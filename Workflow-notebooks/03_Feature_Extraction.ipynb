{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae6a8bda-fa84-4846-9f07-c426f6488c79",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "* **Products used:** \n",
    "[dem_cop_30](https://explorer.digitalearth.africa/products/s2_l2a), [s2_l2a](https://explorer.digitalearth.africa/products/dem_cop_90), [dem_srtm](https://explorer.digitalearth.africa/products/dem_srtm), [dem_srtm_deriv](https://explorer.digitalearth.africa/products/dem_srtm_deriv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d049a6-588a-430b-96a6-6233cc208642",
   "metadata": {},
   "source": [
    "## Background:\n",
    "\n",
    "Training data extraction plays a crucial role in training machine learning models. The process involves extracting relevant feature layers from a geospatial dataset based on predefined geometries or regions of interest. This enables the creation of accurate and reliable classification models for various applications such as land cover mapping, crop monitoring, and environmental analysis.\n",
    "\n",
    "To facilitate this task, the open-data-cube provides a powerful function called \"collect_training_data.\" This function is part of the deafrica_tools.classification script and is specifically designed to extract training data from the open-data-cube using geometries defined within a GeoJSON file. The GeoJSON file contains the spatial boundaries or polygons that delineate the regions of interest for which training data needs to be extracted.\n",
    "\n",
    "## Description:\n",
    "\n",
    "This notebook focuses on the extraction of training data (feature layers) from the open-data-cube using geometries defined within a GeoJSON file. It follows a step-by-step approach to guide users in utilizing the \"collect_training_data\" function effectively. The goal is to enable users to extract the appropriate training data for their specific use case.\n",
    "\n",
    "The main steps in this notebook are as follows:\n",
    "\n",
    "1. **Previewing the Training Data:** The notebook starts by plotting the polygons from the training data on a basemap. This visualization provides users with a visual representation of the regions of interest for which training data will be extracted.\n",
    "\n",
    "2. **Defining the Feature Layer Function:** Next, a feature layer function is defined. This function specifies the set of feature layers to be extracted from the open-data-cube. These layers are carefully selected based on their relevance to the classification task at hand.\n",
    "\n",
    "3. **Extracting Training Data:** The \"collect_training_data\" function is then employed to extract the training data from the datacube. It utilizes the predefined geometries from the GeoJSON file and retrieves the corresponding feature layers. This step ensures that the extracted data aligns precisely with the defined regions of interest.\n",
    "\n",
    "4. **Exporting Training Data:** Finally, the extracted training data is exported and saved to disk. This facilitates its subsequent use in other scripts or machine learning workflows for training classification models.\n",
    "\n",
    "By following the steps outlined in this notebook, users can leverage the \"collect_training_data\" function to efficiently extract training data from the open-data-cube. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f89ea0-5373-455e-a908-8d3233b535ba",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f926deb-9a6f-4ad8-a808-fa1abf7ba1bb",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e9a94-c117-45f6-a4d7-93025def990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import io\n",
    "import os \n",
    "import math\n",
    "import datacube\n",
    "import warnings\n",
    "import rioxarray\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from datacube.testutils.io import rio_slurp_xarray\n",
    "\n",
    "\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.plotting import map_shapefile\n",
    "from deafrica_tools.bandindices import calculate_indices\n",
    "from classification import collect_training_data\n",
    "from externaldrive import list_gdrive, read_tif_from_gdrive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0072cdfd-2116-4354-b1eb-8875a12aa930",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    " * path: The path to the input vector file from which we will extract training data. A default geojson is provided.\n",
    " * field: This is the name of column in your shapefile attribute table that contains the class labels. The class labels must be integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c624d2a-c9ed-4c4b-be9c-d011f424afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a prefix to identify the area of interest in the saved outputs\n",
    "# By assigning the desired prefix, you can easily identify the outputs associated with the specific area of interest.\n",
    "prefix = 'aoi'\n",
    "field = 'class_id'\n",
    "path = f'data/{prefix}_training_samples.geojson'\n",
    "\n",
    "# Load input data shapefile\n",
    "training_points= gpd.read_file(path) \n",
    "training_points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ff005-92c1-4151-a413-65e216412d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a flag to convert to polygons:\n",
    "use_polygons = True\n",
    "\n",
    "if use_polygons:\n",
    "    # Convert from lat,lon to EPSG:6933 (projection in metres)\n",
    "    training_points = training_points.to_crs(\"EPSG:6933\")\n",
    "\n",
    "    # Buffer geometry to get a square - only if trying to sample multiple pixels\n",
    "    buffer_radius_m = 10\n",
    "    training_points.geometry = training_points.geometry.buffer(buffer_radius_m, cap_style=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b4aac-b5ad-47f8-a668-f8d44322310b",
   "metadata": {},
   "source": [
    "#### Plot on interactive map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ac8d79-e930-4823-8f21-7219b78ffcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = training_points\n",
    "training_points.explore(\n",
    "    tiles = \"https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}\", \n",
    "    attr ='Imagery @2022 Landsat/Copernicus, Map data @2022 Google',\n",
    "    popup=True,\n",
    "    cmap='viridis',\n",
    "    style_kwds=dict(radius= 5, color= 'red', fillOpacity= 0.8, fillColor= 'red', weight= 3),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4698dad-8f5e-4f40-9d85-10a8534c95cb",
   "metadata": {},
   "source": [
    "## Defining Query\n",
    "\n",
    "The function `collect_training_data` takes our geojson containing class labels and extracts training data (features) from the datacube over the locations specified by the input geometries. The function will also pre-process our training data by stacking the arrays into a useful format and removing any `NaN` or `inf` values.\n",
    "\n",
    "The below variables can be set within the `collect_training_data` function:\n",
    "\n",
    "* `field`: The name of column in your geojson file attribute table that contains the class labels, which corresponds to the `class_attr` that we defined earlier.\n",
    "* `zonal_stats`: An optional string giving the names of zonal statistics to calculate across each geometry (polygon or point). Default is None (all pixel values are returned). Supported values are 'mean', 'median', 'max', and 'min'.\n",
    "* `dc_query`: A datacube query dictionary for the Open Data Cube query such as `measurements` (the bands to load from the satellite), the `resolution` (the cell size), and the `output_crs` (the output projection). \n",
    "* `feature_func`:  A function for generating feature layers that is applied to the data within the bounds of the input geometry. This function will take the 'dc_query' as the only argument.\n",
    "* `return_coords`: If True, then the training data will contain two extra columns ‘x_coord’ and ‘y_coord’ corresponding to the x,y coordinate of each sample.\n",
    "\n",
    "> Note: `collect_training_data` also has a number of additional parameters for handling ODC I/O read failures, where polygons that return an excessive number of null values can be resubmitted to the multiprocessing queue.  Check out the [docs](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks/blob/83116e80ebb4f8744e3de74e7a713aadd0a7577a/Tools/deafrica_tools/classification.py#L565) to learn more.\n",
    "\n",
    "We will define the first three parameters and describe the `feature_func` seperately in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c26583-4bf7-4a58-ad43-99adfc3426cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up our inputs to collect_training_data\n",
    "zonal_stats = 'mean'\n",
    "\n",
    "# Set up the inputs for the ODC query\n",
    "time = ('2024')\n",
    "\n",
    "resolution = (-10,10)\n",
    "\n",
    "output_crs='epsg:6933'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b98a9f3-add5-4c07-a2cc-cb7eb3f9c45d",
   "metadata": {},
   "source": [
    "Note that we've selected nine spectral bands with spatial resolution no lower than 20 m here for demonstration. However, it is advised that you test and select the bands based on your own classification task. Using the variables above, we can generate a datacube query object from the parameters above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6294abb7-8481-4582-89aa-f86fc918ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'time': time,\n",
    "    'output_crs': output_crs,\n",
    "    'resolution': resolution,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d193b-2c7f-42a9-96b3-7413fe0fe51a",
   "metadata": {},
   "source": [
    "### Defining feature function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e31cfc-58e9-4d0c-8c65-c4494c448aaf",
   "metadata": {},
   "source": [
    "## Defining feature function\n",
    "\n",
    "To create the desired feature layers, we pass instructions to `collect_training_data` through the `feature_func` parameter. The `feature_func` must accept a `dc_query` dictionary, and return a single `xarray.Dataset` or `xarray.DataArray` containing 2D coordinates (i.e x, y - no time dimension). e.g.\n",
    "\n",
    "          def feature_function(query):\n",
    "              dc = datacube.Datacube(app='feature_layers')\n",
    "              ds = dc.load(**query)\n",
    "              ds = ds.mean('time')\n",
    "              return ds\n",
    "\n",
    "Below, we will define a more complicated feature layer function than the brief example shown above. Firstly We will calculate the Normalised Difference Water Index (NDWI), which is commonly used to distinguish between Water and non-water land cover classes. We use the `calculate_indices`function to automatically calculate NDVI for all specified bands. \n",
    "\n",
    "In addition, we'll use temporal signatures to help distinguish wetland classes. To reduce data size while keeping seasonal changes, we are implementing biannual temporal aggregation, i.e. geomedian (sometimes referred to as the 'geometric median') for each pixel location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654e05f-b1eb-4004-aafb-2d61c0755077",
   "metadata": {},
   "source": [
    "### Read terrain indices from a Google Drive Folder. \n",
    "Make sure you have followed the instructions to set up the connection with [Google Drive API using a service account](https://docs.digitalearthafrica.org/en/latest/platform_tools/googledrive_access.html). This code should only be used when the terrain attribute data is located in a Google Drive to save on Sandbox disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef97112-8d63-4c37-961b-ebeac4988a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Capture the list of TIFF files from Google Drive\n",
    "# tif_files = list_gdrive()\n",
    "\n",
    "# # Initialize an empty dataset for merging\n",
    "# terrain_stacked = xr.Dataset()\n",
    "\n",
    "# # Ensure tif_files contains the expected structure\n",
    "# if isinstance(tif_files, list):\n",
    "#     # Filter the TIFF files from the list\n",
    "#     tif_files = [file for file in tif_files if file['name'].endswith('.tif')]\n",
    "\n",
    "#     # Display the TIFF files with their IDs\n",
    "#     if tif_files:\n",
    "#         print(\"Available TIFF files:\")\n",
    "#         for tif in tif_files:\n",
    "#             print(f\"{tif['name']} (ID: {tif['id']})\")\n",
    "\n",
    "#         # Lists to store the data arrays and their extents\n",
    "#         data_arrays = []\n",
    "#         extents = []\n",
    "#         titles = []\n",
    "\n",
    "#         # Read and merge all the TIFF files into the dataset\n",
    "#         for tif in tif_files:\n",
    "#             selected_file_id = tif['id']  # Select the current TIFF file\n",
    "            \n",
    "#             # Read the selected TIFF file from Google Drive\n",
    "#             data_array, transform = read_tif_from_gdrive(selected_file_id)\n",
    "\n",
    "#             # Check if the data was read successfully\n",
    "#             if data_array is not None:\n",
    "#                 print(f\"Data for {tif['name']} read successfully!\")\n",
    "\n",
    "#                 # Convert to a dataset with the filename as the variable name\n",
    "#                 tif_dataset = data_array.to_dataset(name=tif['name'].replace('.tif', ''))\n",
    "\n",
    "#                 # Merge with the existing stacked dataset\n",
    "#                 terrain_stacked = xr.merge([terrain_stacked, tif_dataset], compat='override')\n",
    "\n",
    "#                 # Store the data array and its extent\n",
    "#                 data_arrays.append(data_array)\n",
    "#                 x_min, x_max = transform[2], transform[2] + transform[0] * data_array.shape[1]\n",
    "#                 y_min, y_max = transform[5] + transform[4] * data_array.shape[0], transform[5]\n",
    "#                 extents.append((x_min, x_max, y_min, y_max))\n",
    "#                 titles.append(tif['name'])  # Store the title for the plot\n",
    "\n",
    "#             else:\n",
    "#                 print(f\"Failed to read data for {tif['name']}.\")\n",
    "\n",
    "#         # # Plot all TIFF data in subplots after reading them all\n",
    "#         # # Calculate the number of rows needed\n",
    "#         # num_files = len(data_arrays)\n",
    "#         # num_columns = 4\n",
    "#         # num_rows = math.ceil(num_files / num_columns)\n",
    "        \n",
    "#         # # Create subplots with the desired number of rows and columns\n",
    "#         # fig, axes = plt.subplots(nrows=num_rows, ncols=num_columns, figsize=(15, 5 * num_rows))\n",
    "\n",
    "#         # # Flatten the axes array for easier indexing\n",
    "#         # axes = axes.flatten()\n",
    "        \n",
    "#         # # Loop through the files and plot them\n",
    "#         # for i in range(num_files):\n",
    "#         #     im = axes[i].imshow(data_arrays[i], cmap='gray', extent=extents[i])\n",
    "#         #     axes[i].set_title(titles[i])\n",
    "#         #     axes[i].set_xlabel('X Coordinate')\n",
    "#         #     axes[i].set_ylabel('Y Coordinate')\n",
    "        \n",
    "#         #     # Add a color bar to each subplot\n",
    "#         #     cbar = fig.colorbar(im, ax=axes[i], orientation='vertical', fraction=0.046, pad=0.04)\n",
    "#         #     cbar.set_label('Pixel Value')\n",
    "        \n",
    "#         # # Hide axes for unused subplots if any\n",
    "#         # for i in range(num_files, len(axes)):\n",
    "#         #     axes[i].axis('off')\n",
    "        \n",
    "#         # # Adjust layout to prevent overlap\n",
    "#         # plt.tight_layout()\n",
    "#         # plt.show()\n",
    "\n",
    "#         # Print a summary of the final merged dataset\n",
    "#         print(f\"Final stacked dataset contains {len(terrain_stacked.data_vars)} variables.\")\n",
    "#     else:\n",
    "#         print(\"No TIFF files found.\")\n",
    "# else:\n",
    "#     print(\"Failed to retrieve files from Google Drive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e420a7-9f75-4885-be54-7006834a5c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_layers(query, terrain_stacked=None):\n",
    "    dc = datacube.Datacube(app='feature_layers')\n",
    "    \n",
    "    # --- Sentinel-2 Annual Geomedian and Indices ---\n",
    "    ds = dc.load(\n",
    "        product='gm_s2_annual',\n",
    "        measurements=['blue', 'green', 'red', 'nir_1', 'nir_2', 'swir_1', 'swir_2', 'emad', 'smad', 'bcmad'],\n",
    "        **query\n",
    "    )\n",
    "    ds = calculate_indices(ds, index=['NDVI', 'MNDWI', 'TCW'], drop=False, satellite_mission='s2')\n",
    "    keep_vars = ['NDVI', 'MNDWI', 'TCW', 'emad', 'smad', 'bcmad']\n",
    "    ds = ds[keep_vars]\n",
    "    ds = ds.rename({var: f'Annual_{var}' for var in ds.data_vars})\n",
    "    \n",
    "    # Stack temporal layers\n",
    "    stacked_vars = []\n",
    "    for var in ds.data_vars:\n",
    "        for i in range(ds.sizes['time']):\n",
    "            stacked_vars.append(ds[var].isel(time=i).rename(f'{var}_{i}'))\n",
    "    ds_stacked = xr.merge(stacked_vars, compat='override')\n",
    "\n",
    "    # --- Sentinel-1 monthly mosaic ---\n",
    "    ds_s1 = dc.load(product=\"s1_monthly_mosaic\", measurements=['vv', 'vh'], group_by=\"solar_day\", **query)\n",
    "    ds_s1 = ds_s1.rename({v: f\"sentinel-1_{v}\" for v in ds_s1.data_vars})\n",
    "    median_s1 = ds_s1.median(dim='time')\n",
    "\n",
    "    # --- ALOS PALSAR ---\n",
    "    ds_alos = dc.load(product='alos_palsar_mosaic', measurements=['hh', 'hv'], **{**query, 'time': ('2022')})\n",
    "    ds_alos = ds_alos.rename({v: f\"alos_palsar_{v}\" for v in ds_alos.data_vars})\n",
    "    median_alos = ds_alos.median()\n",
    "\n",
    "    # --- WOfS ---\n",
    "    wofs_annual = dc.load(product='wofs_ls_summary_annual', like=ds.geobox, time=query['time'])\n",
    "    wofs_annual_freq = wofs_annual.frequency.squeeze('time', drop=True).rename('WOfS_annual')\n",
    "\n",
    "    wofs_alltime = dc.load(product='wofs_ls_summary_alltime', like=ds.geobox)\n",
    "    wofs_alltime_freq = wofs_alltime.frequency.squeeze('time', drop=True).rename('WOfS_alltime')\n",
    "\n",
    "    # --- CHIRPS Rainfall ---\n",
    "    chirps = dc.load(product='rainfall_chirps_monthly', like=ds.geobox, time=query['time'])\n",
    "    chirps_mean = chirps.rainfall.mean(dim='time').rename('Chirps_rainfall')\n",
    "\n",
    "    # --- Cropland Probability ---\n",
    "    crop_prob = dc.load(product='crop_mask', like=ds.geobox).prob.rename('Crop_probability')\n",
    "\n",
    "    # --- Land Cover Dataset - ESA Worldcover ---\n",
    "    esa_lulc = dc.load(product='esa_worldcover_2021', measurements='classification', like=ds.geobox)\n",
    "    esa_lulc = esa_lulc.classification.squeeze('time', drop=True).rename('ESA_2021_lulc')\n",
    "\n",
    "\n",
    "    # Option 1: Use this code if the terrain attribute files are located in a folder within the Sandbox.\n",
    "    # Uncomment the code below and comment out the Google Drive option if using this method.\n",
    "    # loop through the terrain attribite files and add them to the dataset\n",
    "    \n",
    "    folder = os.path.join(\"data/terrain_attributes/\", prefix)\n",
    "    for filename in os.listdir(folder):\n",
    "            if filename.endswith('.tif'):\n",
    "                filepath = os.path.join(folder, filename)\n",
    "                tif = rio_slurp_xarray(filepath, gbox=ds.geobox)\n",
    "                tif = tif.to_dataset(name=filename.replace('.tif', ''))\n",
    "                ds_stacked = xr.merge([ds_stacked, tif], compat='override')\n",
    "                \n",
    "    # Option 2: Use this code if the terrain attribute files are in a Google Drive folder\n",
    "    # Uncomment the code below and comment out the Sandbox folder option above if using this method.\n",
    "    # Make sure to change None to terrain_stacked in the function argument at the top if using terrain indices from a google drive folder\n",
    "\n",
    "    # bbox = ds.geobox.extent.boundingbox\n",
    "    # if terrain_stacked is not None:  \n",
    "    #     terrain_stacked.attrs['crs'] = ds.geobox.crs\n",
    "    #     terrain_stacked = terrain_stacked.sel(x=slice(bbox.left, bbox.right), y=slice(bbox.top, bbox.bottom))\n",
    "    #     terrain_stacked = terrain_stacked.rio.reproject_match(ds)\n",
    "    #     ds_stacked = xr.merge([ds_stacked, terrain_stacked], compat='override', combine_attrs='override')\n",
    "    \n",
    "    # --- Merge All Layers ---\n",
    "    ds_stacked = xr.merge([\n",
    "        ds_stacked, median_s1, median_alos,\n",
    "        wofs_annual_freq, wofs_alltime_freq,\n",
    "        chirps_mean, crop_prob, esa_lulc\n",
    "    ], compat='override', combine_attrs='override')\n",
    "    \n",
    "    return ds_stacked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b1adc3-6b86-468d-b01c-9c01da16df06",
   "metadata": {},
   "source": [
    "Now let's run the `collect_training_data` function. This may take minutes to hours depending on your number of training points, number of measurements/bands set for the query and the calculation work in the feature function. Since we've used 10 measurements (9 spectral bands and 1 NDWI index) with 6 temporal geomedian for each band, it can be very time-consuming to finish the training features extraction. Therefore, here we are passing in `gdf=training_points[0:10]` to only run the code over the first 10 geometries as demonstration. Nevertheless, the extracted full training data file is provided in the 'Results/' folder, which will be used for next module of the workflow.\n",
    "\n",
    "> **Note**:  With supervised classification, its common to have many, many labelled geometries in the training data. `collect_training_data` can parallelize across the geometries in order to speed up the extracting of training data. Setting `ncpus>1` will automatically trigger the parallelization. However, its best to set `ncpus=1` to begin with to assist with debugging before triggering the parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbd8ec3-1802-4d0e-8460-03ee03862ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### detect the number of CPUs\n",
    "ncpus=round(get_cpu_quota())\n",
    "print('ncpus = '+str(ncpus))\n",
    "\n",
    "# Ensure the 'field' column is of integer dtype\n",
    "training_points[field] = training_points[field].astype(int)\n",
    "\n",
    "# collect training data\n",
    "column_names, model_input = collect_training_data(\n",
    "    gdf=training_points[0:1],\n",
    "    dc_query=query,\n",
    "    ncpus=1,\n",
    "    field=field, # integer class label\n",
    "    zonal_stats=\"mean\",\n",
    "    feature_func=feature_layers,\n",
    "    return_coords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb6171-c20f-45d9-bf7b-be464c56f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c8587-9c07-4f84-84f7-981fae19e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_training_features=pd.DataFrame(data=model_input,columns=column_names)\n",
    "pd_training_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58724be-7158-420d-b6ac-375db82c11f4",
   "metadata": {},
   "source": [
    "### Export training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dfb224-c1c8-4393-ae6f-6686e2a75d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to geopandas dataframe\n",
    "pd_training_features=pd.DataFrame(data=model_input,columns=column_names)\n",
    "#set the name and location of the output file\n",
    "# output_file = \"results/training_features.txt\"\n",
    "output_file = f\"results/{prefix}_training_features.txt\"\n",
    "#Export files to disk\n",
    "pd_training_features.to_csv(output_file, header=True, index=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c058b89-0556-40a2-99d5-3beb3120a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create geopandas dataframe\n",
    "gpd_training_features=gpd.GeoDataFrame(pd_training_features, \n",
    "geometry=gpd.points_from_xy(model_input[:,-2], model_input[:,-1],crs=output_crs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d67d1-0a31-41a7-80c5-54e7e51087dd",
   "metadata": {},
   "source": [
    "#####  Add a column for binary (wetland/non-wetland) classification\n",
    "This block ensures that both binary and multi-class classification labels are properly set up from the original `class_id` field:\n",
    "\n",
    "- If `class_id` contains only two values (0 and 1), it is assumed to be binary, and is renamed to `class_id_binary`.\n",
    "- If `class_id` includes additional wetland types, a new binary column `class_id_binary` is created:\n",
    "  - `1` for any wetland type (i.e., values not equal to 0)\n",
    "  - `0` for non-wetland (i.e., value equal to 0)\n",
    "  - The original column is then renamed to `class_id_type` for use in multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ec785-2a4c-4d9e-ad91-c7ec1c7b9a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if unique values in 'class_id' are only 0 and 1\n",
    "unique_values = gpd_training_features['class_id'].unique()\n",
    "if len(unique_values) == 2 and set(unique_values) == {0, 1}:\n",
    "    # Replace 'class_id' with 'class_id_binary'\n",
    "    gpd_training_features.rename(columns={'class_id': 'class_id_binary'}, inplace=True)\n",
    "else:\n",
    "    # Create 'class_id_binary' column based on condition\n",
    "    gpd_training_features['class_id_binary'] = gpd_training_features['class_id'].apply(lambda x: 1 if x != 0 else 0)\n",
    "    gpd_training_features.rename(columns={'class_id': 'class_id_type'}, inplace=True)\n",
    "\n",
    "# Insert the new column at the second position\n",
    "gpd_training_features.insert(0, 'class_id_binary', gpd_training_features.pop('class_id_binary'))\n",
    "print(gpd_training_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b22f43-2dba-4d96-a085-23a8ee428338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace non-zero values in the 'class_id' column with 1\n",
    "gpd_training_features['class_id_binary'] = gpd_training_features['class_id_type'].apply(lambda x: 1 if x != 0 else 0)\n",
    "# Insert the new column at the second position\n",
    "gpd_training_features.insert(1, 'class_id_binary', gpd_training_features.pop('class_id_binary'))\n",
    "gpd_training_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc2b5d6-d1bf-4924-b99e-00a59d60fbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as geojson file\n",
    "# gpd_training_features.to_file('results/training_features.geojson', driver=\"GeoJSON\")\n",
    "geojson_file = f\"results/{prefix}_training_features.geojson\"\n",
    "gpd_training_features.to_file(geojson_file, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5532548-5fa3-4e7c-8f74-fc6b3087b006",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c897d749-5f90-4287-a9b2-5a1f1c708008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "datetime.today().strftime('%Y-%m-%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
