{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffca76bc-495d-4e18-a10a-4840d08526b9",
   "metadata": {},
   "source": [
    "# Mapping wetland intrinsic potential and classifying types\n",
    "\n",
    "### Background\n",
    "The main function of this notebook is to utilise the trained Random Forest classifier from Notebook 4 to predict the landscapeâ€™s wetland intrinsic potential from the binary model and then classify wetland areas into classes for a specific area of interest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39902f6-896e-4746-a086-b9929c82cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "import rasterio\n",
    "import datacube\n",
    "import warnings\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "from osgeo import gdal\n",
    "import rioxarray as rxr\n",
    "import geopandas as gpd\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import load\n",
    "from odc.algo import xr_geomedian\n",
    "from odc.dscache.tools import tiling\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import ListedColormap,BoundaryNorm\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.cog import write_cog\n",
    "from odc.geo.geom import Geometry\n",
    "from datacube.utils.geometry import BoundingBox\n",
    "from datacube.testutils.io import rio_slurp_xarray\n",
    "\n",
    "from classification import predict_xr\n",
    "from deafrica_tools.spatial import xr_rasterize\n",
    "from deafrica_tools.dask import create_local_dask_cluster\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.bandindices import calculate_indices\n",
    "from deafrica_tools.plotting import rgb, display_map\n",
    "from deafrica_tools.areaofinterest import define_area\n",
    "from externaldrive import list_gdrive, read_tif_from_gdrive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff83fb0-f9aa-458d-820d-3d841b70091e",
   "metadata": {},
   "source": [
    "## Create Dask cluster for running predictions\n",
    "We use dask to parallel and speed up processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fabf0f-6bcd-4733-ba24-b4973031f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dask cluster\n",
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4197f6b4-86ea-4a7a-b717-352eeb5ad6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app='wetland_classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78b8ce3-0c31-4810-ba6e-e5d44ea2f5c2",
   "metadata": {},
   "source": [
    "## Load the model \n",
    "We use the model trained and saved in the [Train_Classification_Algorithm](04_Train_Classification_Algorithm.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f1f191-87f4-4d96-9568-d4f3e8c31659",
   "metadata": {},
   "source": [
    "### Load area of interest (AOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be56683-7d32-4a57-a094-26d30e202ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'aoi'  # Specify the desired prefix\n",
    "\n",
    "# Use a polygon as a GeoJSON or Esri Shapefile. \n",
    "boundary_file = \"aoi.geojson\" \n",
    "aoi = define_area(vector_path=boundary_file)\n",
    "\n",
    "#Create a geopolygon and geodataframe of the area of interest\n",
    "geom = Geometry(aoi[\"features\"][0][\"geometry\"], crs=\"epsg:4326\")\n",
    "geom_gdf = gpd.GeoDataFrame(geometry=[geom], crs=geom.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f080b65-4f2c-481b-a313-82d73ade07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latitude and longitude range of the geopolygon\n",
    "lat_range = (geom_gdf.total_bounds[1], geom_gdf.total_bounds[3])\n",
    "lon_range = (geom_gdf.total_bounds[0], geom_gdf.total_bounds[2])\n",
    "display_map(x=lon_range, y=lat_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca2b60-0ed6-4707-a8e9-cd744d6a3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if both binary and type models are available\n",
    "if os.path.exists(f'results/{prefix}_Binary_RF_model.joblib') and os.path.exists(f'results/{prefix}_Type_RF_model.joblib'):\n",
    "    binary_model = load(f'results/{prefix}_Binary_RF_model.joblib').set_params(n_jobs=1)\n",
    "    print('Loaded binary random forest model:\\n', binary_model)\n",
    "\n",
    "    type_model = load(f'results/{prefix}_Type_RF_model.joblib').set_params(n_jobs=1)\n",
    "    print('Loaded type random forest model:\\n', type_model)\n",
    "elif os.path.exists(f'results/{prefix}_Binary_RF_model.joblib'):\n",
    "    binary_model = load(f'results/{prefix}_Binary_RF_model.joblib').set_params(n_jobs=1)\n",
    "    print('Loaded binary random forest model:\\n', binary_model)\n",
    "elif os.path.exists(f'results/{prefix}_Type_RF_model.joblib'):\n",
    "    type_model = load(f'results/{prefix}_Type_RF_model.joblib').set_params(n_jobs=1)\n",
    "    print('Loaded type random forest model:\\n', type_model)\n",
    "else:\n",
    "    print(\"No trained models found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c8e0b-9aa6-4273-857d-163bd1b58b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths for importing the feature names\n",
    "binary_features_path = f\"results/{prefix}_binary_model_features.json\"\n",
    "type_features_path = f\"results/{prefix}_type_model_features.json\"\n",
    "\n",
    "# Load binary model feature names from JSON if binary model exists\n",
    "if os.path.exists(binary_features_path):\n",
    "    with open(binary_features_path, \"r\") as binary_file:\n",
    "        binary_features_dict = json.load(binary_file)\n",
    "    binary_feature_names = binary_features_dict[\"features\"]\n",
    "    print(\"Loaded binary model features.\")\n",
    "else:\n",
    "    print(\"No binary model features found.\")\n",
    "\n",
    "# Load type model feature names from JSON if type model exists\n",
    "if os.path.exists(type_features_path):\n",
    "    with open(type_features_path, \"r\") as type_file:\n",
    "        type_features_dict = json.load(type_file)\n",
    "    type_feature_names = type_features_dict[\"features\"]\n",
    "    print(\"Loaded type model features.\")\n",
    "else:\n",
    "    print(\"No type model features found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc7ef5f-8c54-41b7-a7fe-57eb79475e3d",
   "metadata": {},
   "source": [
    "### Break area of interest into tiles for smaller processing chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12287b-a30b-42d4-8d59-1068cf0ef967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gridspec from string and convert to geoms\n",
    "tiles = tiling.parse_gridspec(s=\"epsg:6933;40;1024\")\n",
    "tiles.tile_coords(tile_index=(0, 0))\n",
    "tiles = tiles.tiles_from_geopolygon(geom)\n",
    "geoms = (i[1].extent.geom for i in tiles)\n",
    "\n",
    "geoms_df = gpd.GeoDataFrame(geometry=list(geoms))\n",
    "\n",
    "# Set the CRS to EPSG:6933\n",
    "geoms_df.crs = \"EPSG:6933\"\n",
    "\n",
    "geoms_df.to_file(f\"data/{prefix}_tiles.geojson\", driver=\"GeoJSON\")\n",
    "\n",
    "# Plot the geometries\n",
    "geoms_df.plot()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcdf5f1-e266-4a10-829b-fc7dbcd7f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "geoms_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe6684-d727-49f5-b3a6-2b5f3fe3770c",
   "metadata": {},
   "source": [
    "### Read terrain indices from a Google Drive Folder. \n",
    "Make sure you have followed the instructions to set up the connection with [Google Drive API using a service account](https://docs.digitalearthafrica.org/en/latest/platform_tools/googledrive_access.html). This code should only be used when the terrain attribute data is located in a Google Drive to save on Sandbox disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b1940-2d78-46e8-9c7d-f225cf7facb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Capture the list of TIFF files from Google Drive\n",
    "# tif_files = list_gdrive()\n",
    "\n",
    "# # Initialize an empty dataset for merging\n",
    "# terrain_stacked = xr.Dataset()\n",
    "\n",
    "# # Ensure tif_files contains the expected structure\n",
    "# if isinstance(tif_files, list):\n",
    "#     # Filter the TIFF files from the list\n",
    "#     tif_files = [file for file in tif_files if file['name'].endswith('.tif')]\n",
    "\n",
    "#     # Display the TIFF files with their IDs\n",
    "#     if tif_files:\n",
    "#         print(\"Available TIFF files:\")\n",
    "#         for tif in tif_files:\n",
    "#             print(f\"{tif['name']} (ID: {tif['id']})\")\n",
    "\n",
    "#         # Lists to store the data arrays and their extents\n",
    "#         data_arrays = []\n",
    "#         extents = []\n",
    "#         titles = []\n",
    "\n",
    "#         # Read and merge all the TIFF files into the dataset\n",
    "#         for tif in tif_files:\n",
    "#             selected_file_id = tif['id']  # Select the current TIFF file\n",
    "            \n",
    "#             # Read the selected TIFF file from Google Drive\n",
    "#             data_array, transform = read_tif_from_gdrive(selected_file_id)\n",
    "\n",
    "#             # Check if the data was read successfully\n",
    "#             if data_array is not None:\n",
    "#                 print(f\"Data for {tif['name']} read successfully!\")\n",
    "\n",
    "#                 # Convert to a dataset with the filename as the variable name\n",
    "#                 tif_dataset = data_array.to_dataset(name=tif['name'].replace('.tif', ''))\n",
    "\n",
    "#                 # Merge with the existing stacked dataset\n",
    "#                 terrain_stacked = xr.merge([terrain_stacked, tif_dataset], compat='override')\n",
    "\n",
    "#                 # Store the data array and its extent\n",
    "#                 data_arrays.append(data_array)\n",
    "#                 x_min, x_max = transform[2], transform[2] + transform[0] * data_array.shape[1]\n",
    "#                 y_min, y_max = transform[5] + transform[4] * data_array.shape[0], transform[5]\n",
    "#                 extents.append((x_min, x_max, y_min, y_max))\n",
    "#                 titles.append(tif['name'])  # Store the title for the plot\n",
    "\n",
    "#             else:\n",
    "#                 print(f\"Failed to read data for {tif['name']}.\")\n",
    "\n",
    "#         # Plot all TIFF data in subplots after reading them all\n",
    "#         # Calculate the number of rows needed\n",
    "#         num_files = len(data_arrays)\n",
    "#         num_columns = 4\n",
    "#         num_rows = math.ceil(num_files / num_columns)\n",
    "        \n",
    "#         # Create subplots with the desired number of rows and columns\n",
    "#         fig, axes = plt.subplots(nrows=num_rows, ncols=num_columns, figsize=(15, 5 * num_rows))\n",
    "\n",
    "#         # Flatten the axes array for easier indexing\n",
    "#         axes = axes.flatten()\n",
    "        \n",
    "#         # Loop through the files and plot them\n",
    "#         for i in range(num_files):\n",
    "#             im = axes[i].imshow(data_arrays[i], cmap='gray', extent=extents[i])\n",
    "#             axes[i].set_title(titles[i])\n",
    "#             axes[i].set_xlabel('X Coordinate')\n",
    "#             axes[i].set_ylabel('Y Coordinate')\n",
    "        \n",
    "#             # Add a color bar to each subplot\n",
    "#             cbar = fig.colorbar(im, ax=axes[i], orientation='vertical', fraction=0.046, pad=0.04)\n",
    "#             cbar.set_label('Pixel Value')\n",
    "        \n",
    "#         # Hide axes for unused subplots if any\n",
    "#         for i in range(num_files, len(axes)):\n",
    "#             axes[i].axis('off')\n",
    "        \n",
    "#         # Adjust layout to prevent overlap\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "#         # Print a summary of the final merged dataset\n",
    "#         print(f\"Final stacked dataset contains {len(terrain_stacked.data_vars)} variables.\")\n",
    "#     else:\n",
    "#         print(\"No TIFF files found.\")\n",
    "# else:\n",
    "#     print(\"Failed to retrieve files from Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce884b3-427b-4164-9582-bf3e145fc2c0",
   "metadata": {},
   "source": [
    "## Create the query for running the predictions\n",
    "\n",
    "We use the query saved from the feature extraction notebook to ensure data from the same periods are retrieved. However, only selected features will be used. \n",
    "\n",
    "> We add `dask_chunks` to the query parameter so the data will be lazy-loaded and only the features used by the model will be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd384e48-20ef-4d8e-bc66-6d5a004c66de",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = ('2024')\n",
    "# using nine spectral bands with 10~20 m spatial resolution\n",
    "resolution = (-20,20)\n",
    "output_crs='epsg:6933'\n",
    "\n",
    "def feature_layers(query, terrain_stacked=None):\n",
    "    dc = datacube.Datacube(app='feature_layers')\n",
    "    \n",
    "    # --- Sentinel-2 Annual Geomedian and Indices ---\n",
    "    ds = dc.load(\n",
    "        product='gm_s2_annual',\n",
    "        measurements=['blue', 'green', 'red', 'nir_1', 'nir_2', 'swir_1', 'swir_2', 'emad', 'smad', 'bcmad'],\n",
    "        **query\n",
    "    )\n",
    "    ds = calculate_indices(ds, index=['NDVI', 'MNDWI', 'TCW'], drop=False, satellite_mission='s2')\n",
    "    keep_vars = ['NDVI', 'MNDWI', 'TCW', 'emad', 'smad', 'bcmad']\n",
    "    ds = ds[keep_vars]\n",
    "    ds = ds.rename({var: f'Annual_{var}' for var in ds.data_vars})\n",
    "    \n",
    "    # Stack temporal layers\n",
    "    stacked_vars = []\n",
    "    for var in ds.data_vars:\n",
    "        for i in range(ds.sizes['time']):\n",
    "            stacked_vars.append(ds[var].isel(time=i).rename(f'{var}_{i}'))\n",
    "    ds_stacked = xr.merge(stacked_vars, compat='override')\n",
    "\n",
    "    # --- Sentinel-1 monthly mosaic ---\n",
    "    ds_s1 = dc.load(product=\"s1_monthly_mosaic\", measurements=['vv', 'vh'], group_by=\"solar_day\", **query)\n",
    "    ds_s1 = ds_s1.rename({v: f\"sentinel-1_{v}\" for v in ds_s1.data_vars})\n",
    "    median_s1 = ds_s1.median(dim='time')\n",
    "\n",
    "    # --- ALOS PALSAR ---\n",
    "    ds_alos = dc.load(product='alos_palsar_mosaic', measurements=['hh', 'hv'], **query)\n",
    "    ds_alos = ds_alos.rename({v: f\"alos_palsar_{v}\" for v in ds_alos.data_vars})\n",
    "\n",
    "    # --- WOfS ---\n",
    "    wofs_annual = dc.load(product='wofs_ls_summary_annual', like=ds.geobox, time=query['time'])\n",
    "    wofs_annual_freq = wofs_annual.frequency.squeeze('time', drop=True).rename('WOfS_annual')\n",
    "\n",
    "    wofs_alltime = dc.load(product='wofs_ls_summary_alltime', like=ds.geobox)\n",
    "    wofs_alltime_freq = wofs_alltime.frequency.squeeze('time', drop=True).rename('WOfS_alltime')\n",
    "\n",
    "    # --- CHIRPS Rainfall ---\n",
    "    chirps = dc.load(product='rainfall_chirps_monthly', like=ds.geobox, time=query['time'])\n",
    "    chirps_mean = chirps.rainfall.mean(dim='time').rename('Chirps_rainfall')\n",
    "\n",
    "    # --- Cropland Probability ---\n",
    "    crop_prob = dc.load(product='crop_mask', like=ds.geobox).prob.rename('Crop_probability')\n",
    "\n",
    "    # --- Land Cover Dataset - ESA Worldcover ---\n",
    "    esa_lulc = dc.load(product='esa_worldcover_2021', measurements='classification', like=ds.geobox)\n",
    "    esa_lulc = esa_lulc.classification.squeeze('time', drop=True).rename('ESA_2021_lulc')\n",
    "\n",
    "    # Option 1: Use this code if the terrain attribute files are located in a folder within the Sandbox.\n",
    "    # Uncomment the code below and comment out the Google Drive option if using this method.\n",
    "    # loop through the terrain attribite files and add them to the dataset\n",
    "    \n",
    "    folder = os.path.join(\"data/terrain_attributes/\", prefix)\n",
    "    for filename in os.listdir(folder):\n",
    "            if filename.endswith('.tif'):\n",
    "                filepath = os.path.join(folder, filename)\n",
    "                tif = rio_slurp_xarray(filepath, gbox=ds.geobox)\n",
    "                tif = tif.to_dataset(name=filename.replace('.tif', ''))\n",
    "                ds_stacked = xr.merge([ds_stacked, tif], compat='override')\n",
    "                \n",
    "    # Option 2: Use this code if the terrain attribute files are in a Google Drive folder\n",
    "    # Uncomment the code below and comment out the Sandbox folder option above if using this method.\n",
    "    # Make sure to change None to terrain_stacked in the function argument at the top if using terrain indices from a google drive folder\n",
    "\n",
    "    # bbox = ds.geobox.extent.boundingbox\n",
    "    # if terrain_stacked is not None:  \n",
    "    #     terrain_stacked.attrs['crs'] = ds.geobox.crs\n",
    "    #     terrain_stacked = terrain_stacked.sel(x=slice(bbox.left, bbox.right), y=slice(bbox.top, bbox.bottom))\n",
    "    #     terrain_stacked = terrain_stacked.rio.reproject_match(ds)\n",
    "    #     ds_stacked = xr.merge([ds_stacked, terrain_stacked], compat='override', combine_attrs='override')\n",
    "    \n",
    "    # --- Merge All Layers ---\n",
    "    ds_stacked = xr.merge([\n",
    "        ds_stacked, median_s1, ds_alos,\n",
    "        wofs_annual_freq, wofs_alltime_freq,\n",
    "        chirps_mean, crop_prob, esa_lulc\n",
    "    ], compat='override', combine_attrs='override')\n",
    "    \n",
    "    return ds_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4035da-63d8-45a4-9492-fc25e2bd4f85",
   "metadata": {},
   "source": [
    "## Apply classification model to predict wetlands in the AOI\n",
    "\n",
    "The model will be applied over each tile, producing a prediction map and a probabilities map. The maps are saved as Cloud-Optimized Geotiffs (COGs).\n",
    "\n",
    "> Tiles are processed in sequence. For each tile, the processing needs to fit into the compute resources available in the sandbox. Make the tile size smaller if you run out of memory. For production of a map over a large region or country, consider applying for [a large sandbox (with more CPUs and momery)](\n",
    "https://helpdesk.digitalearthafrica.org/portal/en/community/topic/call-for-application-for-access-to-large-sandboxes-15-processing-cores-and-120-gb-of-memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e409f83-f633-445f-8e22-639b4f578111",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_existing = False\n",
    "output_folder = \"results\"\n",
    "binary_tiles_pred_folder = os.path.join(output_folder, f\"{prefix}/binary_tiles_predicted\")\n",
    "os.makedirs(binary_tiles_pred_folder, exist_ok=True)\n",
    "dask_chunks = {'x': 2500, 'y': 2500}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39037b1-b35d-4b09-a7ef-10419564b00c",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "#### Binary predictions and probabilities per tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c3715-add1-4e78-ac3d-4e80ba5ece1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "# Generate a datacube query object\n",
    "query = {\n",
    "    'geopolygon': geom,\n",
    "    'time': time,\n",
    "    'resolution': resolution,\n",
    "    'output_crs': output_crs,\n",
    "    'dask_chunks': dask_chunks,\n",
    "}\n",
    "\n",
    "for index in range(len(geoms_df)):   # Iterate over the tiles\n",
    "    aoi = geoms_df.iloc[index]\n",
    "    progress_text = f\"Predicting... Polygon {index + 1} of {len(geoms_df)}\"\n",
    "    print(progress_text)\n",
    "    \n",
    "    # Check if polygon has already been processed. If so, skip\n",
    "    output_filename = os.path.join(\n",
    "        binary_tiles_pred_folder, f\"{prefix}_tile{index:03d}_wetland_binary_prediction.tif\")\n",
    "    probabilities_filename = os.path.join(\n",
    "        binary_tiles_pred_folder, f\"{prefix}_tile{index:03d}_wetland_binary_probabilities.tif\")\n",
    "    if skip_existing and os.path.exists(output_filename) and os.path.exists(probabilities_filename):\n",
    "        print(\"Completed; Skipping\")\n",
    "        continue\n",
    "\n",
    "    # Set up query based on AOI geometry\n",
    "    geom = geometry.Geometry(geom=aoi.geometry, crs=geoms_df.crs)\n",
    "    query.update({\"geopolygon\": geom})\n",
    "    \n",
    "    # Calculate features\n",
    "    data = feature_layers(query)\n",
    "    \n",
    "    # Only keep features that are in the original list of columns\n",
    "    data = data[binary_feature_names].chunk(dask_chunks)\n",
    "    \n",
    "    # Predict using the imported model\n",
    "    predicted = predict_xr(\n",
    "        binary_model,\n",
    "        data,\n",
    "        proba=True,\n",
    "        clean=True,\n",
    "        return_input=True\n",
    "    ).persist()\n",
    "    \n",
    "    # Create a mask for the AOI\n",
    "    print(\"    Getting AOI mask\")\n",
    "    aoi_mask = xr_rasterize(\n",
    "        gdf=gpd.GeoDataFrame({\"Polygon\": [index], \"geometry\": [aoi.geometry]}, crs=geoms_df.crs),\n",
    "        da=predicted,\n",
    "        crs=output_crs,\n",
    "    )\n",
    "\n",
    "    # Set the no data value\n",
    "    NODATA = 255\n",
    "\n",
    "    # Mask the predictions\n",
    "    print(\"    Preparing predictions\")\n",
    "    predicted_masked = (\n",
    "        predicted.Predictions.where(aoi_mask == 1, NODATA)\n",
    "    ).compute()\n",
    "    predicted_masked.attrs[\"nodata\"] = NODATA\n",
    "\n",
    "    # Write predictions to COG\n",
    "    print(f\"    Writing predictions to {output_filename}\")\n",
    "    write_cog(\n",
    "        predicted_masked,\n",
    "        fname=output_filename,\n",
    "        overwrite=True,\n",
    "        nodata=NODATA,\n",
    "    )\n",
    "\n",
    "    # Mask the probabilities\n",
    "    print(\"    Preparing probabilities\")\n",
    "    probability_masked = (\n",
    "        predicted.Probabilities[..., :, 1].where(aoi_mask == 1, NODATA) * 100\n",
    "    ).compute()\n",
    "    probability_masked.attrs[\"nodata\"] = NODATA\n",
    "    probability_masked = probability_masked.where((probability_masked >= 0) & (probability_masked <= 100))\n",
    "\n",
    "    print(f\"    Writing probabilities to {probabilities_filename}\")\n",
    "    write_cog(\n",
    "        probability_masked,\n",
    "        fname=probabilities_filename,\n",
    "        overwrite=True,\n",
    "        nodata= NODATA\n",
    "    )\n",
    "\n",
    "    # Clear variables to free memory\n",
    "    del predicted, predicted_masked, probability_masked, aoi_mask, data\n",
    "    # gc.collect()  # Call garbage collection to free memory\n",
    "\n",
    "    # Clear the output\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa0c0d3-48ba-44ab-887e-bc39db44e8a0",
   "metadata": {},
   "source": [
    "#### Merge the tiles and export the final wetland predictions and probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7375118-b8cf-4dd9-9402-f8515ad8400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_tiles = f\"{binary_tiles_pred_folder}/{prefix}_tile*_wetland_binary_prediction.tif\"\n",
    "out_mosaic_prediction = f\"{output_folder}/{prefix}/{prefix}_wetland_binary_prediction.tif\"\n",
    "\n",
    "# Remove the merged file if it already exists\n",
    "if os.path.exists(out_mosaic_prediction): \n",
    "    subprocess.run(f\"rm {out_mosaic_prediction}\", shell=True)\n",
    "\n",
    "gdal_cmd = f\"gdalwarp -of GTiff -cutline {boundary_file} -crop_to_cutline -co COMPRESS=DEFLATE {prediction_tiles} {out_mosaic_prediction}\"\n",
    "process = subprocess.Popen(gdal_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "print(\"Processing wetland binary prediction:\")\n",
    "for line in process.stdout:\n",
    "    print(f\"\\r{line.strip()}\", end='', flush=True)  \n",
    "\n",
    "process.wait()  \n",
    "print(\"\\rWetland binary prediction completed.\")  \n",
    "\n",
    "# Repeat for the probabilities\n",
    "probabilities_tiles = f\"{binary_tiles_pred_folder}/{prefix}_tile*_wetland_binary_probabilities.tif\"\n",
    "out_mosaic_probabilities = f\"{output_folder}/{prefix}/{prefix}_wetland_binary_probabilities.tif\"\n",
    "\n",
    "# Remove the merged file if it already exists\n",
    "if os.path.exists(out_mosaic_probabilities): \n",
    "    subprocess.run(f\"rm {out_mosaic_probabilities}\", shell=True)\n",
    "\n",
    "gdal_cmd = f\"gdalwarp -of GTiff -cutline {boundary_file} -crop_to_cutline -co COMPRESS=DEFLATE {probabilities_tiles} {out_mosaic_probabilities}\"\n",
    "process = subprocess.Popen(gdal_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "print(\"Processing wetland binary probabilities:\")\n",
    "for line in process.stdout:\n",
    "    print(f\"\\r{line.strip()}\", end='', flush=True)  \n",
    "\n",
    "process.wait()  # Wait for the process to finish\n",
    "print(\"\\rWetland binary probabilities completed.\") \n",
    "\n",
    "# Load the merged predictions and probababilities as xarray DataArray\n",
    "binary_wetland_predictions = rxr.open_rasterio(out_mosaic_prediction).squeeze()\n",
    "binary_wetland_probabilities = rxr.open_rasterio(out_mosaic_probabilities).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f549f44-af9d-4211-88f4-f81552c60082",
   "metadata": {},
   "source": [
    "#### Plot the wetland predicition and probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c44a25-0535-4c65-849e-9b2356b14aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new label dictionary\n",
    "labels_dict_binary = {'Non-wetland': 0, 'Wetland': 1}\n",
    "\n",
    "# Generate random colors for each class (excluding class 0)\n",
    "random.seed(42)  # Set a seed for reproducibility\n",
    "class_colors = {class_name: f'#{random.randint(0, 255):02x}{random.randint(0, 255):02x}{random.randint(0, 255):02x}'\n",
    "                for class_name in labels_dict_binary}\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))  # Adjust the figure size if needed\n",
    "\n",
    "# Plot wetland predictions\n",
    "sorted_classes = sorted(labels_dict_binary, key=lambda x: labels_dict_binary[x])\n",
    "cmap = ListedColormap([class_colors[class_name] for class_name in sorted_classes])\n",
    "\n",
    "# Plot wetland predictions\n",
    "binary_wetland_predictions.plot.imshow(ax=axes[0],\n",
    "                                        cmap=cmap,\n",
    "                                        add_colorbar=False,\n",
    "                                        interpolation='none')  \n",
    "axes[0].set_title('Wetland Predictions', fontweight='bold')\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# Plot clipped wetland probabilities\n",
    "im = binary_wetland_probabilities.plot.imshow(ax=axes[1],\n",
    "                                               cmap='RdYlBu',\n",
    "                                               add_colorbar=False,\n",
    "                                               interpolation='none')\n",
    "axes[1].set_title('Wetland Probabilities', fontweight='bold')\n",
    "axes[1].set_aspect('equal') \n",
    "\n",
    "# Add legend to the first subplot\n",
    "patches_list = [Patch(facecolor=class_colors[class_name]) for class_name in sorted_classes]\n",
    "legend = axes[0].legend(patches_list, [class_name for class_name in sorted_classes],\n",
    "                        loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Adjust spacing\n",
    "plt.subplots_adjust(wspace=0.4) \n",
    "\n",
    "# Add colorbar outside the subplot\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), orientation='vertical', fraction=0.05, pad=0.04)\n",
    "cbar.set_label('Wetland Probability')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3002ca8-44a2-46df-b5a8-c6db86350289",
   "metadata": {},
   "source": [
    "### Independent accuaracy assessment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b6bc2-5f01-4576-a342-97dc00e6da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the testing points GeoDataFrame\n",
    "testing_points = gpd.read_file(f'data/{prefix}_testing_samples.geojson')\n",
    "\n",
    "# Replace non-zero values in the 'class_id' column with 1\n",
    "testing_points['class_id_binary'] = testing_points['class_id'].apply(lambda x: 1 if x != 0 else 0)\n",
    "# Insert the new column at the second position\n",
    "testing_points.insert(1, 'class_id_binary', testing_points.pop('class_id_binary'))\n",
    "\n",
    "# Sample the predictions and probabilities at testing points\n",
    "sampled_predictions = []\n",
    "sampled_probabilities = []\n",
    "\n",
    "for point in testing_points.geometry:\n",
    "    # Extract the point coordinates\n",
    "    x, y = point.x, point.y\n",
    "    \n",
    "    # Sample the binary predictions and probabilities at the point coordinates\n",
    "    prediction = binary_wetland_predictions.sel(x=x, y=y, method='nearest').item()\n",
    "    probability = binary_wetland_probabilities.sel(x=x, y=y, method='nearest').item()\n",
    "    \n",
    "    # Append the sampled values\n",
    "    sampled_predictions.append(prediction)\n",
    "    sampled_probabilities.append(probability)\n",
    "\n",
    "# Add the sampled values as new columns in the GeoDataFrame\n",
    "testing_points['sampled_predictions'] = sampled_predictions\n",
    "testing_points['sampled_probabilities'] = sampled_probabilities\n",
    "\n",
    "# Calculate the number of correctly classified samples\n",
    "correct_count = (testing_points['sampled_predictions'] == testing_points['class_id_binary']).sum()\n",
    "\n",
    "# Calculate the total number of samples\n",
    "total_count = len(testing_points)\n",
    "\n",
    "# Calculate the accuracy as a percentage\n",
    "accuracy_percentage = (correct_count / total_count) * 100\n",
    "\n",
    "# Print the accuracy percentage\n",
    "print(f\"Overall Accuracy: {accuracy_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a27929e-9659-457c-9e07-cce33e811ef9",
   "metadata": {},
   "source": [
    "## Wetland Type classification\n",
    "#### Applied to the wetland class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb02cb0-dbc8-47f4-841e-341cdddf570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholded_binary_wetland_probabilities = binary_wetland_probabilities.where(binary_wetland_probabilities >= 50)\n",
    "\n",
    "# Check if type_model exists\n",
    "if 'type_model' in locals():   \n",
    "    tiles_pred_folder = os.path.join(output_folder, f\"{prefix}/type_tiles_predicted\")\n",
    "    os.makedirs(tiles_pred_folder, exist_ok=True)\n",
    "\n",
    "    predictions = []\n",
    "    dask_chunks = {'x':2500,'y':2500}\n",
    "\n",
    "    # generate a datacube query object\n",
    "    query = {\n",
    "        'geopolygon': geom,\n",
    "        'time': time,\n",
    "        'resolution': resolution,\n",
    "        'output_crs': output_crs,\n",
    "        'dask_chunks': dask_chunks,\n",
    "    }\n",
    "\n",
    "    # for index in range(len(geoms_df)):   # Iterate over the tiles\n",
    "    for index in range(len(geoms_df)):    \n",
    "        aoi = geoms_df.iloc[index]\n",
    "    #     print(f\"Processing Polygon {index + 1} of {len(geoms_df)}\")\n",
    "        progress_text = f\"Predicting... Polygon {index + 1} of {len(geoms_df)}\"\n",
    "        print(progress_text)\n",
    "\n",
    "        # Check if polygon has already been processed. If so, skip\n",
    "        output_filename = os.path.join(\n",
    "            tiles_pred_folder, f\"{prefix}_tile{index:03d}_wetland_type_prediction.tif\")\n",
    "        probabilities_filename = os.path.join(\n",
    "            tiles_pred_folder, f\"{prefix}_tile{index:03d}_wetland_type_probabilities.tif\")\n",
    "        if skip_existing and os.path.exists(output_filename) and os.path.exists(probabilities_filename):\n",
    "            print(\"Completed; Skipping\")\n",
    "            continue\n",
    "\n",
    "        # set up query based on aoi geometry\n",
    "        geom = geometry.Geometry(geom=aoi.geometry, crs=geoms_df.crs)\n",
    "        query.update({\"geopolygon\": geom})\n",
    "\n",
    "        # set the no data value\n",
    "        NODATA = 255\n",
    "\n",
    "        # calculate features\n",
    "        data = feature_layers(query).persist()\n",
    "\n",
    "        # Clip the data to the extent of wetland predictions\n",
    "#         data = data.where(binary_wetland_predictions == 1)\n",
    "        data = data.where(thresholded_binary_wetland_probabilities != 0)\n",
    "\n",
    "        # Only keep features that are in the original list of columns\n",
    "        data = data[type_feature_names]\n",
    "\n",
    "        # Convert xarray Dataset object to Dask array\n",
    "        data_dask = data.chunk(dask_chunks)\n",
    "\n",
    "        # predict using the imported model\n",
    "        predicted = predict_xr(type_model,\n",
    "                               data_dask,\n",
    "                               proba=True,\n",
    "                               persist=True,\n",
    "                               clean=True,\n",
    "                               return_input=True\n",
    "                               ).compute().persist()\n",
    "\n",
    "        # Create a mask for the aoi\n",
    "        print(\"    Getting AOI mask\")\n",
    "        aoi_mask = xr_rasterize(\n",
    "            gdf=gpd.GeoDataFrame({\"Polygon\": [index], \"geometry\": [\n",
    "                                 aoi.geometry]}, crs=geoms_df.crs),\n",
    "            da=predicted,\n",
    "            crs=output_crs,\n",
    "        )\n",
    "\n",
    "\n",
    "        # Mask the predictions with both AOI mask and wetland predictions mask\n",
    "        print(\"    Preparing predictions\")\n",
    "        predicted_masked = (\n",
    "            predicted.Predictions.where((aoi_mask == 1) & (binary_wetland_predictions == 1), NODATA)\n",
    "        )\n",
    "        predicted_masked.attrs[\"nodata\"] = NODATA\n",
    "        predicted_masked = predicted_masked.where(thresholded_binary_wetland_probabilities != 0)\n",
    "                                                  \n",
    "        # Write predictions to COG\n",
    "        print(f\"    Writing predictions to {output_filename}\")\n",
    "        write_cog(\n",
    "            predicted_masked,\n",
    "            fname=output_filename,\n",
    "            overwrite=True,\n",
    "            nodata=255,\n",
    "        )\n",
    "        \n",
    "        # Mask the probabilities with both AOI mask and wetland predictions mask\n",
    "        probability_masked = (\n",
    "            predicted.Probabilities.max().where((aoi_mask == 1) & (binary_wetland_predictions == 1), NODATA) * 100\n",
    "        )\n",
    "        probability_masked.attrs[\"nodata\"] = NODATA\n",
    "        probability_masked = probability_masked.where((probability_masked >= 0) & (probability_masked <= 100))\n",
    "\n",
    "        print(f\"    Writing probabilities to {probabilities_filename}\")\n",
    "        write_cog(\n",
    "            probability_masked,\n",
    "            fname=probabilities_filename,\n",
    "            overwrite=True,\n",
    "            nodata=255,\n",
    "        )\n",
    "\n",
    "        # Clear variables to free memory\n",
    "        del predicted, predicted_masked, probability_masked, aoi_mask, data\n",
    "        gc.collect()  # Call garbage collection to free memory\n",
    "    \n",
    "        # Clear the output\n",
    "        clear_output(wait=True)\n",
    "else:\n",
    "    print(\"Skipping prediction process as wetland type model is not available.\")             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f3add8-d9f1-4307-9257-877f2f09f5d4",
   "metadata": {},
   "source": [
    "#### Merge the tiles and export the final wetland predicitons and probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf76e9-03db-4989-a68c-ca4e5be87a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_pred_folder = os.path.join(output_folder, f\"{prefix}/type_tiles_predicted\")\n",
    "# Check if type_model exists\n",
    "if 'type_model' in locals():\n",
    "    prediction_tiles = f\"{tiles_pred_folder}/{prefix}_tile*_wetland_type_prediction.tif\"\n",
    "    out_mosaic_prediction = f\"{output_folder}/{prefix}/{prefix}_wetland_type_prediction.tif\"\n",
    "    \n",
    "    # Remove the merged file if it already exists\n",
    "    if os.path.exists(out_mosaic_prediction): \n",
    "        subprocess.run(f\"rm {out_mosaic_prediction}\", shell=True)\n",
    "    \n",
    "    gdal_cmd = f\"gdalwarp -of GTiff -cutline {boundary_file} -crop_to_cutline -co COMPRESS=DEFLATE -dstnodata 255 {prediction_tiles} {out_mosaic_prediction}\"\n",
    "    process = subprocess.Popen(gdal_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    \n",
    "    print(\"Processing wetland type prediction:\")\n",
    "    for line in process.stdout:\n",
    "        print(f\"\\r{line.strip()}\", end='', flush=True)  \n",
    "    \n",
    "    process.wait()  \n",
    "    print(\"\\rWetland type prediction completed.\")  \n",
    "    \n",
    "    # # Repeat for the probabilities\n",
    "    probabilities_tiles = f\"{tiles_pred_folder}/{prefix}_tile*_wetland_type_probabilities.tif\"\n",
    "    out_mosaic_probabilities = f\"{output_folder}/{prefix}/{prefix}_wetland_type_probabilities.tif\"\n",
    "    \n",
    "    # Remove the merged file if it already exists\n",
    "    if os.path.exists(out_mosaic_probabilities): \n",
    "        subprocess.run(f\"rm {out_mosaic_probabilities}\", shell=True)\n",
    "    \n",
    "    gdal_cmd = f\"gdalwarp -of GTiff -cutline {boundary_file} -crop_to_cutline -co COMPRESS=DEFLATE -dstnodata 255 {probabilities_tiles} {out_mosaic_probabilities}\"\n",
    "    process = subprocess.Popen(gdal_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    \n",
    "    print(\"Processing wetland type probabilities:\")\n",
    "    for line in process.stdout:\n",
    "        print(f\"\\r{line.strip()}\", end='', flush=True)  \n",
    "    \n",
    "    process.wait()  # Wait for the process to finish\n",
    "    print(\"\\rWetland type probabilities completed.\") \n",
    "\n",
    "\n",
    "# Load the merged predictions and probababilities as xarray DataArray\n",
    "# Mask NoData in the merged predictions and probabilities\n",
    "wetland_type_predictions = rxr.open_rasterio(out_mosaic_prediction, masked=True).squeeze().where(lambda x: x != 255)\n",
    "wetland_type_probabilities = rxr.open_rasterio(out_mosaic_probabilities, masked=True).squeeze().where(lambda x: x != 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479d3190-ef86-4f49-a93e-e0ae1e660eb3",
   "metadata": {},
   "source": [
    "#### Plot the wetland type predicitions and probabilitiesÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7200fa6-6a8b-464d-960b-707c2de3ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if type_model exists\n",
    "if 'type_model' in locals():\n",
    "    # Import the class label dictionary\n",
    "    with open(f'data/{prefix}_labels_dict.json', 'r') as json_file:\n",
    "        labels_dict = json.load(json_file)\n",
    "        # Remove the non-wetland class from the dictionary\n",
    "        if 'Non-wetland' in labels_dict:\n",
    "            del labels_dict['Non-wetland']    \n",
    "\n",
    "    # Generate random colors for each class (excluding class 0)\n",
    "    random.seed(42)  # Set a seed for reproducibility\n",
    "    class_colors = {class_name: f'#{random.randint(0, 255):02x}{random.randint(0, 255):02x}{random.randint(0, 255):02x}'\n",
    "                    for class_name in labels_dict}\n",
    "\n",
    "    # Create the figure and axes\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Sort classes based on their numeric labels\n",
    "    sorted_classes = sorted(labels_dict, key=lambda x: labels_dict[x])\n",
    "\n",
    "    # Define color map for the wetland type predictions\n",
    "    cmap = ListedColormap([class_colors[class_name] for class_name in sorted_classes])\n",
    "\n",
    "    # Plot wetland type predictions\n",
    "    wetland_type_predictions.plot.imshow(ax=axes[0],\n",
    "                                         cmap=cmap,\n",
    "                                         add_colorbar=False,\n",
    "                                         interpolation='none')\n",
    "    axes[0].set_title('Wetland Types', fontweight='bold')\n",
    "    axes[0].set_aspect('equal')\n",
    "\n",
    "    # Plot clipped wetland probabilities\n",
    "    im = wetland_type_probabilities.plot.imshow(ax=axes[1],\n",
    "                                                cmap='RdYlBu',\n",
    "                                                add_colorbar=False,\n",
    "                                                interpolation='none')\n",
    "    axes[1].set_title('Wetland Probabilities', fontweight='bold')\n",
    "    axes[1].set_aspect('equal')\n",
    "\n",
    "    # Add legend to the first subplot\n",
    "    patches_list = [Patch(facecolor=class_colors[class_name]) for class_name in sorted_classes]\n",
    "    legend = axes[0].legend(patches_list, [class_name for class_name in sorted_classes],\n",
    "                            loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    # Adjust spacing\n",
    "    plt.subplots_adjust(wspace=0.4) \n",
    "\n",
    "    # Add colorbar outside the subplot\n",
    "    cbar = fig.colorbar(im, ax=axes.ravel().tolist(), orientation='vertical', fraction=0.05, pad=0.04)\n",
    "    cbar.set_label('Wetland Probability')\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping plotting process as wetland type model is not available.\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e92ef-e1b0-4afc-98fc-fdd5ae2d9a97",
   "metadata": {},
   "source": [
    "### Independent accuaracy assessment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fdaffe-0733-4e4c-b387-9a427a161f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if type_model exists\n",
    "if 'type_model' in locals():\n",
    "# Load the testing points GeoDataFrame\n",
    "    testing_points = gpd.read_file(f'data/{prefix}_testing_samples.geojson')\n",
    "\n",
    "    # Sample the predictions and probabilities at testing points\n",
    "    sampled_predictions = []\n",
    "    sampled_probabilities = []\n",
    "\n",
    "    for point in testing_points.geometry:\n",
    "        # Extract the point coordinates\n",
    "        x, y = point.x, point.y\n",
    "\n",
    "        # Sample the binary predictions at the point coordinates\n",
    "        prediction = wetland_type_predictions.sel(x=x, y=y, method='nearest').item()\n",
    "\n",
    "        # Append the sampled values\n",
    "        sampled_predictions.append(prediction)\n",
    "\n",
    "    # Add the sampled values as new columns in the GeoDataFrame\n",
    "    testing_points['sampled_predictions'] = sampled_predictions\n",
    "    \n",
    "    # Remove rows with NaN values for sampled predictions\n",
    "    testing_points = testing_points.dropna(subset=['sampled_predictions'])\n",
    "\n",
    "    # Calculate the number of correctly classified samples\n",
    "    correct_count = (testing_points['sampled_predictions'] == testing_points['class_id']).sum()\n",
    "\n",
    "    # Calculate the total number of samples\n",
    "    total_count = len(testing_points)\n",
    "\n",
    "    # Calculate the accuracy as a percentage\n",
    "    accuracy_percentage = (correct_count / total_count) * 100\n",
    "\n",
    "    # Print the accuracy percentage\n",
    "    print(f\"Overall Accuracy: {accuracy_percentage:.2f}%\")\n",
    "else:\n",
    "    print(\"Skipping accuracy calculation as wetland type model is not available.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c45799-be8a-40ce-b13f-2ee9f9234eba",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ec171-625f-44a6-9e07-aee0c4a771c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "datetime.today().strftime('%Y-%m-%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
