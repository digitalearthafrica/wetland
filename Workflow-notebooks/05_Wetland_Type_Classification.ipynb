{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffca76bc-495d-4e18-a10a-4840d08526b9",
   "metadata": {},
   "source": [
    "# Create a wetland type map\n",
    "\n",
    "### Background\n",
    "This notebook can be used to generate wetland type map over a region defined by a vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39902f6-896e-4746-a086-b9929c82cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "import rasterio\n",
    "import datacube\n",
    "import warnings\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "from osgeo import gdal\n",
    "import rioxarray as rxr\n",
    "import geopandas as gpd\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import load\n",
    "from odc.algo import xr_geomedian\n",
    "from odc.dscache.tools import tiling\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import ListedColormap,BoundaryNorm\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.cog import write_cog\n",
    "from datacube.utils.geometry import Geometry\n",
    "from datacube.utils.geometry import BoundingBox, Geometry\n",
    "from datacube.testutils.io import rio_slurp_xarray\n",
    "\n",
    "from classification import predict_xr\n",
    "from deafrica_tools.spatial import xr_rasterize\n",
    "from deafrica_tools.dask import create_local_dask_cluster\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.bandindices import calculate_indices\n",
    "from deafrica_tools.plotting import rgb, display_map\n",
    "from deafrica_tools.areaofinterest import define_area\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff83fb0-f9aa-458d-820d-3d841b70091e",
   "metadata": {},
   "source": [
    "## Create Dask cluster for running predictions\n",
    "We use dask to parallel and speed up processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fabf0f-6bcd-4733-ba24-b4973031f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dask cluster\n",
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4197f6b4-86ea-4a7a-b717-352eeb5ad6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app='wetland_classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78b8ce3-0c31-4810-ba6e-e5d44ea2f5c2",
   "metadata": {},
   "source": [
    "## Load the model \n",
    "We use the model trained and saved in the [Train_Classification_Algorithm](04_Train_Classification_Algorithm.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f1f191-87f4-4d96-9568-d4f3e8c31659",
   "metadata": {},
   "source": [
    "### Load area of interest (AOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be56683-7d32-4a57-a094-26d30e202ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'Test'  # Specify the desired prefix - name of area of interest (aoi) is best to ensure the files have the aoi prefix when saved\n",
    "\n",
    "# Use a polygon as a GeoJSON or Esri Shapefile. \n",
    "boundary_file = \"data/aoi/test.geojson\" \n",
    "aoi = define_area(vector_path=boundary_file)\n",
    "\n",
    "#Create a geopolygon and geodataframe of the area of interest\n",
    "geom = Geometry(aoi[\"features\"][0][\"geometry\"], crs=\"epsg:4326\")\n",
    "geom_gdf = gpd.GeoDataFrame(geometry=[geom], crs=geom.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f080b65-4f2c-481b-a313-82d73ade07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latitude and longitude range of the geopolygon\n",
    "lat_range = (geom_gdf.total_bounds[1], geom_gdf.total_bounds[3])\n",
    "lon_range = (geom_gdf.total_bounds[0], geom_gdf.total_bounds[2])\n",
    "display_map(x=lon_range, y=lat_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca2b60-0ed6-4707-a8e9-cd744d6a3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if both binary and type models are available\n",
    "if os.path.exists(f'results/{prefix}_Binary_RF_model.joblib') and os.path.exists(f'results/{prefix}_Type_RF_model.joblib'):\n",
    "    binary_model = load(f'results/{prefix}_Binary_RF_model.joblib').set_params(n_jobs=1)\n",
    "    print('Loaded binary random forest model:\\n', binary_model)\n",
    "\n",
    "    type_model = load(f'results/{prefix}_Type_RF_model.joblib').set_params(n_jobs=1)\n",
    "    print('Loaded type random forest model:\\n', type_model)\n",
    "elif os.path.exists(f'results/{prefix}_Binary_RF_model.joblib'):\n",
    "    binary_model = load(f'results/{prefix}_Binary_RF_model.joblib').set_params(n_jobs=1)\n",
    "    print('Loaded binary random forest model:\\n', binary_model)\n",
    "elif os.path.exists(f'results/{prefix}_Type_RF_model.joblib'):\n",
    "    type_model = load(f'results/{prefix}_Type_RF_model.joblib').set_params(n_jobs=1)\n",
    "    print('Loaded type random forest model:\\n', type_model)\n",
    "else:\n",
    "    print(\"No trained models found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c8e0b-9aa6-4273-857d-163bd1b58b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths for importing the feature names\n",
    "binary_features_path = f\"results/{prefix}_binary_model_features.json\"\n",
    "type_features_path = f\"results/{prefix}_type_model_features.json\"\n",
    "\n",
    "# Load binary model feature names from JSON if binary model exists\n",
    "if os.path.exists(binary_features_path):\n",
    "    with open(binary_features_path, \"r\") as binary_file:\n",
    "        binary_features_dict = json.load(binary_file)\n",
    "    binary_feature_names = binary_features_dict[\"features\"]\n",
    "    print(\"Loaded binary model features.\")\n",
    "else:\n",
    "    print(\"No binary model features found.\")\n",
    "\n",
    "# Load type model feature names from JSON if type model exists\n",
    "if os.path.exists(type_features_path):\n",
    "    with open(type_features_path, \"r\") as type_file:\n",
    "        type_features_dict = json.load(type_file)\n",
    "    type_feature_names = type_features_dict[\"features\"]\n",
    "    print(\"Loaded type model features.\")\n",
    "else:\n",
    "    print(\"No type model features found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e4075e-e7a9-4001-9ade-8a849e9350cc",
   "metadata": {},
   "source": [
    "### Break area of interest into tiles for smaller processing chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e520a5-bffa-4d9d-9541-644c43400981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gridspec from string and convert to geoms\n",
    "tiles = tiling.parse_gridspec(s=\"epsg:6933;40;1024\")\n",
    "tiles.tile_coords(tile_index=(0, 0))\n",
    "tiles = tiles.tiles_from_geopolygon(geom)\n",
    "geoms = (i[1].extent.geom for i in tiles)\n",
    "\n",
    "geoms_df = gpd.GeoDataFrame(geometry=list(geoms))\n",
    "\n",
    "# Set the CRS to EPSG:6933\n",
    "geoms_df.crs = \"EPSG:6933\"\n",
    "\n",
    "geoms_df.to_file(f\"data/{prefix}_tiles.geojson\", driver=\"GeoJSON\")\n",
    "\n",
    "# Plot the geometries\n",
    "geoms_df.plot()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce884b3-427b-4164-9582-bf3e145fc2c0",
   "metadata": {},
   "source": [
    "## Create the query for running the predictions\n",
    "\n",
    "We use the query saved from the feature extraction notebook to ensure data from the same periods are retrieved. However, only selected features will be used. \n",
    "\n",
    "> We add `dask_chunks` to the query parameter so the data will be lazy-loaded and only the features used by the model will be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65b75f-5276-42d1-823a-908aa2bc70f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = ('2022')\n",
    "# using nine spectral bands with 10~20 m spatial resolution\n",
    "resolution = (-20,20)\n",
    "output_crs='epsg:6933'\n",
    "\n",
    "def feature_layers(query):\n",
    "    # connect to the datacube\n",
    "#     dc = datacube.Datacube(app='feature_layers')\n",
    "    \n",
    "    # load s2 annual geomedian\n",
    "    ds = dc.load(\n",
    "        product='gm_s2_annual',\n",
    "        measurements = ['blue','green','red','red_edge_1','red_edge_2', 'red_edge_3','nir_1','nir_2','swir_1','swir_2','emad','smad','bcmad'],\n",
    "        **query)\n",
    "    \n",
    "    # calculate some band indices\n",
    "    ds = calculate_indices(ds,\n",
    "                           index=['NDVI', 'MNDWI','TCW'],\n",
    "                           drop=False,\n",
    "                           satellite_mission='s2')\n",
    "    \n",
    "    \n",
    "    # Add a prefix \"Annual\" to the band names\n",
    "    new_band_names = ['Annual_' + band_name for band_name in ds.data_vars]\n",
    "    ds = ds.rename({old_band_name: new_band_name for old_band_name, new_band_name in zip(ds.data_vars, new_band_names)})\n",
    "\n",
    "    # stack multi-temporal measurements and rename them\n",
    "    n_time = ds.sizes['time']\n",
    "    list_measurements = list(ds.keys())\n",
    "    list_stack_measures = []\n",
    "    for j in range(len(list_measurements)):\n",
    "        for k in range(n_time):\n",
    "            variable_name = list_measurements[j]+'_'+str(k)\n",
    "            measure_single = ds[list_measurements[j]].isel(time=k).rename(variable_name)\n",
    "            list_stack_measures.append(measure_single)\n",
    "    ds_stacked = xr.merge(list_stack_measures, compat='override')\n",
    "\n",
    "    \n",
    "    # Load the Sentinel-1 data    \n",
    "    ds_s1 = dc.load(product=[\"s1_rtc\"],\n",
    "                  measurements=['vv', 'vh'],\n",
    "                  group_by=\"solar_day\",\n",
    "                  **query\n",
    "                 )\n",
    "    # Add a prefix \"Sent1_\" to the variables in ds_s1\n",
    "    ds_s1 = ds_s1.rename({old_var: 'sentinel-1_' + old_var for old_var in ds_s1.data_vars})\n",
    "\n",
    "    # median values are used to scale the measurements so they have a similar range for visualization\n",
    "    median_s1 = ds_s1[['sentinel-1_vv','sentinel-1_vh']].median(dim='time')\n",
    "\n",
    "    # Add ALOS L-Band Annual mosaic\n",
    "    ds_alos = dc.load(product='alos_palsar_mosaic',\n",
    "                      measurements=['hh','hv'],\n",
    "                      **query)\n",
    "    \n",
    "#   Add a prefix \"alos_palsar\" to the variables in ds_alos\n",
    "    ds_alos = ds_alos.rename({old_var: 'alos_palsar_' + old_var for old_var in ds_alos.data_vars})  \n",
    "    median_alos = ds_alos[['alos_palsar_hh','alos_palsar_hv']].median(dim='time')\n",
    "    \n",
    "\n",
    "    # Add WOfS Annual summary\n",
    "    wofs_annual = dc.load(product='wofs_ls_summary_annual',\n",
    "               like=ds.geobox,\n",
    "               time=query['time'])\n",
    "    wofs_annual_frequency = wofs_annual.frequency\n",
    "    wofs_annual_frequency.name = 'WOfS'\n",
    "    \n",
    "    \n",
    "    # loop through the terrain attribite files and add them to the dataset\n",
    "    folder = os.path.join(\"data/terrain_attributes/\", prefix)\n",
    "    for filename in os.listdir(folder):\n",
    "            if filename.endswith('.tif'):\n",
    "                filepath = os.path.join(folder, filename)\n",
    "                tif = rio_slurp_xarray(filepath, gbox=ds.geobox)\n",
    "                tif = tif.to_dataset(name=filename.replace('.tif', ''))\n",
    "                ds_stacked = xr.merge([ds_stacked, tif], compat='override')\n",
    "\n",
    "\n",
    "    # merge all the datasets into a single dataset\n",
    "    ds_stacked = xr.merge([ds_stacked, median_s1, median_alos, wofs_annual_frequency], compat='override')\n",
    "\n",
    "    return ds_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4035da-63d8-45a4-9492-fc25e2bd4f85",
   "metadata": {},
   "source": [
    "## Apply classification model to predict wetlands in the AOI\n",
    "\n",
    "The model will be applied over each tile, producing a prediction map and a probabilities map. The maps are saved as Cloud-Optimized Geotiffs (COGs).\n",
    "\n",
    "> Tiles are processed in sequence. For each tile, the processing needs to fit into the compute resources available in the sandbox. Make the tile size smaller if you run out of memory. For production of a map over a large region or country, consider applying for [a large sandbox (with more CPUs and momery)](\n",
    "https://helpdesk.digitalearthafrica.org/portal/en/community/topic/call-for-application-for-access-to-large-sandboxes-15-processing-cores-and-120-gb-of-memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e409f83-f633-445f-8e22-639b4f578111",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_existing = False\n",
    "output_folder = \"results\"\n",
    "binary_tiles_pred_folder = os.path.join(output_folder, f\"{prefix}/binary_tiles_predicted\")\n",
    "os.makedirs(binary_tiles_pred_folder, exist_ok=True)\n",
    "dask_chunks = {'x': 2500, 'y': 2500}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39037b1-b35d-4b09-a7ef-10419564b00c",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "#### Binary predictions and probabilities per tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c3715-add1-4e78-ac3d-4e80ba5ece1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "dask_chunks = {'x': 2500, 'y': 2500}\n",
    "\n",
    "# Generate a datacube query object\n",
    "query = {\n",
    "    'geopolygon': geom,\n",
    "    'time': time,\n",
    "    'resolution': resolution,\n",
    "    'output_crs': output_crs,\n",
    "    'dask_chunks': dask_chunks,\n",
    "}\n",
    "\n",
    "for index in range(len(geoms_df)):   # Iterate over the tiles\n",
    "    aoi = geoms_df.iloc[index]\n",
    "    progress_text = f\"Predicting... Polygon {index + 1} of {len(geoms_df)}\"\n",
    "    print(progress_text)\n",
    "    \n",
    "    # Check if polygon has already been processed. If so, skip\n",
    "    output_filename = os.path.join(\n",
    "        binary_tiles_pred_folder, f\"{prefix}_tile{index:03d}_wetland_binary_prediction.tif\")\n",
    "    probabilities_filename = os.path.join(\n",
    "        binary_tiles_pred_folder, f\"{prefix}_tile{index:03d}_wetland_binary_probabilities.tif\")\n",
    "    if skip_existing and os.path.exists(output_filename) and os.path.exists(probabilities_filename):\n",
    "        print(\"Completed; Skipping\")\n",
    "        continue\n",
    "\n",
    "    # Set up query based on AOI geometry\n",
    "    geom = geometry.Geometry(geom=aoi.geometry, crs=geoms_df.crs)\n",
    "    query.update({\"geopolygon\": geom})\n",
    "    \n",
    "    # Calculate features\n",
    "    data = feature_layers(query).persist()\n",
    "    \n",
    "    # Only keep features that are in the original list of columns\n",
    "    data = data[binary_feature_names].chunk(dask_chunks)\n",
    "\n",
    "    # Predict using the imported model\n",
    "    predicted = predict_xr(\n",
    "        binary_model,\n",
    "        data,\n",
    "        proba=True,\n",
    "        persist=True,\n",
    "        clean=True,\n",
    "        return_input=True\n",
    "    ).persist()\n",
    "    \n",
    "    # Create a mask for the AOI\n",
    "    print(\"    Getting AOI mask\")\n",
    "    aoi_mask = xr_rasterize(\n",
    "        gdf=gpd.GeoDataFrame({\"Polygon\": [index], \"geometry\": [aoi.geometry]}, crs=geoms_df.crs),\n",
    "        da=predicted,\n",
    "        crs=output_crs,\n",
    "    )\n",
    "\n",
    "    # Set the no data value\n",
    "    NODATA = 255\n",
    "\n",
    "    # Mask the predictions\n",
    "    print(\"    Preparing predictions\")\n",
    "    predicted_masked = (\n",
    "        predicted.Predictions.where(aoi_mask == 1, NODATA)\n",
    "    ).compute()\n",
    "    predicted_masked.attrs[\"nodata\"] = NODATA\n",
    "\n",
    "    # Write predictions to COG\n",
    "    print(f\"    Writing predictions to {output_filename}\")\n",
    "    write_cog(\n",
    "        predicted_masked,\n",
    "        fname=output_filename,\n",
    "        overwrite=True,\n",
    "        nodata=NODATA,\n",
    "    )\n",
    "\n",
    "    # Mask the probabilities\n",
    "    print(\"    Preparing probabilities\")\n",
    "    probability_masked = (\n",
    "        predicted.Probabilities[..., :, 1].where(aoi_mask == 1, NODATA) * 100\n",
    "    ).compute()\n",
    "    probability_masked.attrs[\"nodata\"] = NODATA\n",
    "    probability_masked = probability_masked.where((probability_masked >= 0) & (probability_masked <= 100))\n",
    "\n",
    "    print(f\"    Writing probabilities to {probabilities_filename}\")\n",
    "    write_cog(\n",
    "        probability_masked,\n",
    "        fname=probabilities_filename,\n",
    "        overwrite=True,\n",
    "        nodata= NODATA\n",
    "    )\n",
    "\n",
    "    # Clear variables to free memory\n",
    "    del predicted, predicted_masked, probability_masked, aoi_mask, data\n",
    "    gc.collect()  # Call garbage collection to free memory\n",
    "\n",
    "    # Clear the output\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa0c0d3-48ba-44ab-887e-bc39db44e8a0",
   "metadata": {},
   "source": [
    "#### Merge the tiles and export the final wetland predictions and probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7375118-b8cf-4dd9-9402-f8515ad8400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_tiles = f\"{tiles_pred_folder}/{prefix}_tile*_wetland_binary_prediction.tif\"\n",
    "out_mosaic_prediction = f\"{output_folder}/{prefix}/{prefix}_wetland_binary_prediction.tif\"\n",
    "\n",
    "# Remove the merged file if it already exists\n",
    "if os.path.exists(out_mosaic_prediction): \n",
    "    subprocess.run(f\"rm {out_mosaic_prediction}\", shell=True)\n",
    "\n",
    "gdal_cmd = f\"gdalwarp -of GTiff -cutline {boundary_file} -crop_to_cutline -co COMPRESS=DEFLATE {prediction_tiles} {out_mosaic_prediction}\"\n",
    "process = subprocess.Popen(gdal_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "print(\"Processing wetland binary prediction:\")\n",
    "for line in process.stdout:\n",
    "    print(f\"\\r{line.strip()}\", end='', flush=True)  \n",
    "\n",
    "process.wait()  \n",
    "print(\"\\rWetland binary prediction completed.\")  \n",
    "\n",
    "# Repeat for the probabilities\n",
    "probabilities_tiles = f\"{tiles_pred_folder}/{prefix}_tile*_wetland_binary_probabilities.tif\"\n",
    "out_mosaic_probabilities = f\"{output_folder}/{prefix}/{prefix}_wetland_binary_probabilities.tif\"\n",
    "\n",
    "# Remove the merged file if it already exists\n",
    "if os.path.exists(out_mosaic_probabilities): \n",
    "    subprocess.run(f\"rm {out_mosaic_probabilities}\", shell=True)\n",
    "\n",
    "gdal_cmd = f\"gdalwarp -of GTiff -cutline {boundary_file} -crop_to_cutline -co COMPRESS=DEFLATE {probabilities_tiles} {out_mosaic_probabilities}\"\n",
    "process = subprocess.Popen(gdal_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "print(\"Processing wetland binary probabilities:\")\n",
    "for line in process.stdout:\n",
    "    print(f\"\\r{line.strip()}\", end='', flush=True)  \n",
    "\n",
    "process.wait()  # Wait for the process to finish\n",
    "print(\"\\rWetland binary probabilities completed.\") \n",
    "\n",
    "# Load the merged predictions and probababilities as xarray DataArray\n",
    "merged_prediction = rxr.open_rasterio(out_mosaic_prediction).squeeze()\n",
    "merged_probabilities = rxr.open_rasterio(out_mosaic_probabilities).squeeze()\n",
    "\n",
    "# Rasterize the area of interest polygon\n",
    "aoi_raster = xr_rasterize(gdf=geom_gdf,\n",
    "                          da=merged_prediction,\n",
    "                          crs=merged_prediction.rio.crs)\n",
    "\n",
    "\n",
    "# Mask the wetland classes pixels within the AOI\n",
    "binary_wetland_predictions = merged_prediction.where((~np.isnan(merged_prediction)) & (aoi_raster != 0))\n",
    "\n",
    "# Define the output clipped filenames\n",
    "clipped_prediction_file = f\"{output_folder}/{prefix}/{prefix}_wetland_binary_prediction.tif\"\n",
    "clipped_probabilities_file = f\"{output_folder}/{prefix}/{prefix}_wetland_binary_probabilities.tif\"\n",
    "\n",
    "# Write the clipped wetland predictions to file\n",
    "write_cog(binary_wetland_predictions, fname=clipped_prediction_file, overwrite=True)\n",
    "\n",
    "# Clip the wetland probabilities to the AOI\n",
    "binary_wetland_probabilities = merged_probabilities.where((~np.isnan(merged_probabilities)) & (merged_probabilities != 255) & (aoi_raster != 0))\n",
    "\n",
    "# Write the clipped wetland probabilities to file\n",
    "write_cog(binary_wetland_probabilities, fname=clipped_probabilities_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f549f44-af9d-4211-88f4-f81552c60082",
   "metadata": {},
   "source": [
    "#### Plot the wetland predicition and probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c44a25-0535-4c65-849e-9b2356b14aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new label dictionary\n",
    "labels_dict_binary = {'Non-wetland': 0, 'Wetland': 1}\n",
    "\n",
    "# Generate random colors for each class (excluding class 0)\n",
    "random.seed(42)  # Set a seed for reproducibility\n",
    "class_colors = {class_name: f'#{random.randint(0, 255):02x}{random.randint(0, 255):02x}{random.randint(0, 255):02x}'\n",
    "                for class_name in labels_dict_binary}\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))  # Adjust the figure size if needed\n",
    "\n",
    "# Plot wetland predictions\n",
    "sorted_classes = sorted(labels_dict_binary, key=lambda x: labels_dict_binary[x])\n",
    "cmap = ListedColormap([class_colors[class_name] for class_name in sorted_classes])\n",
    "\n",
    "# Plot wetland predictions\n",
    "binary_wetland_predictions.plot.imshow(ax=axes[0],\n",
    "                                        cmap=cmap,\n",
    "                                        add_colorbar=False,\n",
    "                                        interpolation='none')  \n",
    "axes[0].set_title('Wetland Predictions', fontweight='bold')\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# Plot clipped wetland probabilities\n",
    "im = binary_wetland_probabilities.plot.imshow(ax=axes[1],\n",
    "                                               cmap='RdYlBu',\n",
    "                                               add_colorbar=False,\n",
    "                                               interpolation='none')\n",
    "axes[1].set_title('Wetland Probabilities', fontweight='bold')\n",
    "axes[1].set_aspect('equal') \n",
    "\n",
    "# Add legend to the first subplot\n",
    "patches_list = [Patch(facecolor=class_colors[class_name]) for class_name in sorted_classes]\n",
    "legend = axes[0].legend(patches_list, [class_name for class_name in sorted_classes],\n",
    "                        loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Adjust spacing\n",
    "plt.subplots_adjust(wspace=0.4) \n",
    "\n",
    "# Add colorbar outside the subplot\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist(), orientation='vertical', fraction=0.05, pad=0.04)\n",
    "cbar.set_label('Wetland Probability')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3002ca8-44a2-46df-b5a8-c6db86350289",
   "metadata": {},
   "source": [
    "### Independent accuaracy assessment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b6bc2-5f01-4576-a342-97dc00e6da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the testing points GeoDataFrame\n",
    "testing_points = gpd.read_file(f'data/{prefix}_testing_samples.geojson')\n",
    "\n",
    "# Replace non-zero values in the 'class_id' column with 1\n",
    "testing_points['class_id_binary'] = testing_points['class_id'].apply(lambda x: 1 if x != 0 else 0)\n",
    "# Insert the new column at the second position\n",
    "testing_points.insert(1, 'class_id_binary', testing_points.pop('class_id_binary'))\n",
    "\n",
    "# Sample the predictions and probabilities at testing points\n",
    "sampled_predictions = []\n",
    "sampled_probabilities = []\n",
    "\n",
    "for point in testing_points.geometry:\n",
    "    # Extract the point coordinates\n",
    "    x, y = point.x, point.y\n",
    "    \n",
    "    # Sample the binary predictions and probabilities at the point coordinates\n",
    "    prediction = binary_wetland_predictions.sel(x=x, y=y, method='nearest').item()\n",
    "    probability = binary_wetland_probabilities.sel(x=x, y=y, method='nearest').item()\n",
    "    \n",
    "    # Append the sampled values\n",
    "    sampled_predictions.append(prediction)\n",
    "    sampled_probabilities.append(probability)\n",
    "\n",
    "# Add the sampled values as new columns in the GeoDataFrame\n",
    "testing_points['sampled_predictions'] = sampled_predictions\n",
    "testing_points['sampled_probabilities'] = sampled_probabilities\n",
    "\n",
    "# Calculate the number of correctly classified samples\n",
    "correct_count = (testing_points['sampled_predictions'] == testing_points['class_id_binary']).sum()\n",
    "\n",
    "# Calculate the total number of samples\n",
    "total_count = len(testing_points)\n",
    "\n",
    "# Calculate the accuracy as a percentage\n",
    "accuracy_percentage = (correct_count / total_count) * 100\n",
    "\n",
    "# Print the accuracy percentage\n",
    "print(f\"Overall Accuracy: {accuracy_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a27929e-9659-457c-9e07-cce33e811ef9",
   "metadata": {},
   "source": [
    "## Wetland Type classification\n",
    "#### Applied to the wetland class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb02cb0-dbc8-47f4-841e-341cdddf570f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Check if type_model exists\n",
    "if 'type_model' in locals():   \n",
    "    tiles_pred_folder = os.path.join(output_folder, f\"{prefix}/type_tiles_predicted\")\n",
    "    os.makedirs(tiles_pred_folder, exist_ok=True)\n",
    "\n",
    "    predictions = []\n",
    "    dask_chunks = {'x':2500,'y':2500}\n",
    "\n",
    "    # generate a datacube query object\n",
    "    query = {\n",
    "        'geopolygon': geom,\n",
    "        'time': time,\n",
    "        'resolution': resolution,\n",
    "        'output_crs': output_crs,\n",
    "        'dask_chunks': dask_chunks,\n",
    "    }\n",
    "\n",
    "    for index in range(len(geoms_df)):   # Iterate over the tiles\n",
    "        aoi = geoms_df.iloc[index]\n",
    "    #     print(f\"Processing Polygon {index + 1} of {len(geoms_df)}\")\n",
    "        progress_text = f\"Predicting... Polygon {index + 1} of {len(geoms_df)}\"\n",
    "        print(progress_text)\n",
    "\n",
    "        # Check if polygon has already been processed. If so, skip\n",
    "        output_filename = os.path.join(\n",
    "            tiles_pred_folder, f\"{prefix}_tile{index:03d}_wetland_type_prediction.tif\")\n",
    "        probabilities_filename = os.path.join(\n",
    "            tiles_pred_folder, f\"{prefix}_tile{index:03d}_wetland_type_probabilities.tif\")\n",
    "        if skip_existing and os.path.exists(output_filename) and os.path.exists(probabilities_filename):\n",
    "            print(\"Completed; Skipping\")\n",
    "            continue\n",
    "\n",
    "        # set up query based on aoi geometry\n",
    "        geom = geometry.Geometry(geom=aoi.geometry, crs=geoms_df.crs)\n",
    "        query.update({\"geopolygon\": geom})\n",
    "\n",
    "        # set the no data value\n",
    "        NODATA = 255\n",
    "\n",
    "        # calculate features\n",
    "        data = feature_layers(query).persist()\n",
    "\n",
    "        # Clip the data to the extent of wetland predictions\n",
    "#         data = data.where(binary_wetland_predictions == 1)\n",
    "        data = data.where(binary_wetland_probabilities > 50)\n",
    "\n",
    "        # Only keep features that are in the original list of columns\n",
    "        data = data[type_feature_names]\n",
    "\n",
    "        # Convert xarray Dataset object to Dask array\n",
    "        data_dask = data.chunk(dask_chunks)\n",
    "\n",
    "        # predict using the imported model\n",
    "        predicted = predict_xr(type_model,\n",
    "                               data_dask,\n",
    "                               proba=True,\n",
    "                               persist=True,\n",
    "                               clean=True,\n",
    "                               return_input=True\n",
    "                               ).compute().persist()\n",
    "\n",
    "        # Create a mask for the aoi\n",
    "        print(\"    Getting AOI mask\")\n",
    "        aoi_mask = xr_rasterize(\n",
    "            gdf=gpd.GeoDataFrame({\"Polygon\": [index], \"geometry\": [\n",
    "                                 aoi.geometry]}, crs=geoms_df.crs),\n",
    "            da=predicted,\n",
    "            crs=output_crs,\n",
    "        )\n",
    "\n",
    "\n",
    "        # Mask the predictions with both AOI mask and wetland predictions mask\n",
    "        print(\"    Preparing predictions\")\n",
    "        predicted_masked = (\n",
    "            predicted.Predictions.where((aoi_mask == 1) & (binary_wetland_predictions == 1), NODATA)\n",
    "        )\n",
    "        predicted_masked.attrs[\"nodata\"] = NODATA\n",
    "\n",
    "        # Write predictions to COG\n",
    "        print(f\"    Writing predictions to {output_filename}\")\n",
    "        write_cog(\n",
    "            predicted_masked,\n",
    "            fname=output_filename,\n",
    "            overwrite=True,\n",
    "            nodata=255,\n",
    "        )\n",
    "        \n",
    "        # Mask the probabilities with both AOI mask and wetland predictions mask\n",
    "        probability_masked = (\n",
    "            predicted.Probabilities.max().where((aoi_mask == 1) & (binary_wetland_predictions == 1), NODATA) * 100\n",
    "        )\n",
    "        probability_masked.attrs[\"nodata\"] = NODATA\n",
    "\n",
    "        print(f\"    Writing probabilities to {probabilities_filename}\")\n",
    "        write_cog(\n",
    "            probability_masked,\n",
    "            fname=probabilities_filename,\n",
    "            overwrite=True,\n",
    "            nodata=255,\n",
    "        )\n",
    "\n",
    "        # Clear variables to free memory\n",
    "        del predicted, predicted_masked, probability_masked, aoi_mask, data\n",
    "        gc.collect()  # Call garbage collection to free memory\n",
    "    \n",
    "        # Clear the output\n",
    "        clear_output(wait=True)\n",
    "else:\n",
    "    print(\"Skipping prediction process as wetland type model is not available.\")             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f3add8-d9f1-4307-9257-877f2f09f5d4",
   "metadata": {},
   "source": [
    "#### Merge the tiles and export the final wetland predicitons and probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf76e9-03db-4989-a68c-ca4e5be87a65",
   "metadata": {},
   "outputs": [],
   "source": [
    " tiles_pred_folder = os.path.join(output_folder, f\"{prefix}/type_tiles_predicted\")\n",
    "# Check if type_model exists\n",
    "if 'type_model' in locals():\n",
    "    prediction_tiles = f\"{tiles_pred_folder}/{prefix}_tile*_wetland_type_prediction.tif\"\n",
    "    out_mosaic_prediction = f\"{output_folder}/{prefix}/{prefix}_wetland_type_prediction.tif\"\n",
    "    \n",
    "    # Remove the merged file if it already exists\n",
    "    if os.path.exists(out_mosaic_prediction): \n",
    "        subprocess.run(f\"rm {out_mosaic_prediction}\", shell=True)\n",
    "    \n",
    "    gdal_cmd = f\"gdalwarp -of GTiff -cutline {boundary_file} -crop_to_cutline -co COMPRESS=DEFLATE {prediction_tiles} {out_mosaic_prediction}\"\n",
    "    process = subprocess.Popen(gdal_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    \n",
    "    print(\"Processing wetland type prediction:\")\n",
    "    for line in process.stdout:\n",
    "        print(f\"\\r{line.strip()}\", end='', flush=True)  \n",
    "    \n",
    "    process.wait()  \n",
    "    print(\"\\rWetland type prediction completed.\")  \n",
    "    \n",
    "    # # Repeat for the probabilities\n",
    "    probabilities_tiles = f\"{tiles_pred_folder}/{prefix}_tile*_wetland_type_probabilities.tif\"\n",
    "    out_mosaic_probabilities = f\"{output_folder}/{prefix}/{prefix}_wetland_type_probabilities.tif\"\n",
    "    \n",
    "    # Remove the merged file if it already exists\n",
    "    if os.path.exists(out_mosaic_probabilities): \n",
    "        subprocess.run(f\"rm {out_mosaic_probabilities}\", shell=True)\n",
    "    \n",
    "    gdal_cmd = f\"gdalwarp -of GTiff -cutline {boundary_file} -crop_to_cutline -co COMPRESS=DEFLATE {probabilities_tiles} {out_mosaic_probabilities}\"\n",
    "    process = subprocess.Popen(gdal_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    \n",
    "    print(\"Processing wetland type probabilities:\")\n",
    "    for line in process.stdout:\n",
    "        print(f\"\\r{line.strip()}\", end='', flush=True)  \n",
    "    \n",
    "    process.wait()  # Wait for the process to finish\n",
    "    print(\"\\rWetland type probabilities completed.\") \n",
    "\n",
    "\n",
    "# Load the merged predictions and probababilities as xarray DataArray\n",
    "merged_prediction = rxr.open_rasterio(out_mosaic_prediction).squeeze()\n",
    "merged_probabilities = rxr.open_rasterio(out_mosaic_probabilities).squeeze()\n",
    "\n",
    "# Rasterize the area of interest polygon\n",
    "aoi_raster = xr_rasterize(gdf=geom_gdf,\n",
    "                          da=merged_prediction,\n",
    "                          crs=merged_prediction.rio.crs)\n",
    "\n",
    "\n",
    "# Mask the wetland classes pixels within the AOI\n",
    "wetland_type_predictions = merged_prediction.where((~np.isnan(merged_prediction)) & (aoi_raster != 0))\n",
    "\n",
    "# Define the output clipped filenames\n",
    "clipped_prediction_file = f\"{output_folder}/{prefix}/{prefix}_wetland_type_prediction.tif\"\n",
    "clipped_probabilities_file = f\"{output_folder}/{prefix}/{prefix}_wetland_type_probabilities.tif\"\n",
    "\n",
    "# Write the clipped wetland predictions to file\n",
    "write_cog(wetland_type_predictions, fname=clipped_prediction_file, overwrite=True)\n",
    "\n",
    "# Clip the wetland probabilities to the AOI\n",
    "wetland_type_probabilities = merged_probabilities.where((~np.isnan(merged_probabilities)) & (merged_probabilities != 255) & (aoi_raster != 0))\n",
    "\n",
    "# Write the clipped wetland probabilities to file\n",
    "write_cog(wetland_type_probabilities, fname=clipped_probabilities_file, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479d3190-ef86-4f49-a93e-e0ae1e660eb3",
   "metadata": {},
   "source": [
    "#### Plot the wetland type predicitions and probabilities¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a2341-9f6d-4f1a-8187-a2947cc8231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if type_model exists\n",
    "if 'type_model' in locals():\n",
    "    # Import the class label dictionary\n",
    "    with open(f'data/{prefix}_labels_dict.json', 'r') as json_file:\n",
    "        labels_dict = json.load(json_file)\n",
    "        # Remove the non-wetland class from the dictionary\n",
    "        if 0 in labels_dict.values():\n",
    "            del labels_dict['Non-wetland']    \n",
    "\n",
    "    # Generate random colors for each class (excluding class 0)\n",
    "    random.seed(42)  # Set a seed for reproducibility\n",
    "    class_colors = {class_name: f'#{random.randint(0, 255):02x}{random.randint(0, 255):02x}{random.randint(0, 255):02x}'\n",
    "                    for class_name in labels_dict}\n",
    "\n",
    "    # Create the figure and axes\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Plot wetland predictions\n",
    "    # Sort classes based on their numeric labels\n",
    "    sorted_classes = sorted(labels_dict, key=lambda x: labels_dict[x])\n",
    "\n",
    "    # Plot wetland predictions\n",
    "    cmap = ListedColormap([class_colors[class_name] for class_name in sorted_classes])\n",
    "    wetland_type_predictions.plot.imshow(ax=axes[0],\n",
    "                                    cmap=cmap,\n",
    "                                    add_colorbar=False,\n",
    "                                    interpolation='none')\n",
    "    axes[0].set_title('Wetland Types', fontweight='bold')\n",
    "\n",
    "    # Plot clipped wetland probabilities\n",
    "    im = wetland_type_probabilities.plot.imshow(ax=axes[1],\n",
    "                                           cmap='RdYlBu',\n",
    "                                           add_colorbar=False)\n",
    "    axes[1].set_title('Wetland Probabilities', fontweight='bold')\n",
    "\n",
    "    # Add legend to the first subplot\n",
    "    patches_list = [Patch(facecolor=class_colors[class_name]) for class_name in sorted_classes]\n",
    "    legend = axes[0].legend(patches_list, [class_name for class_name in sorted_classes],\n",
    "                            loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    # Adjust spacing\n",
    "    fig.tight_layout()\n",
    "    # Adjust spacing\n",
    "    plt.subplots_adjust(wspace=0.4) \n",
    "\n",
    "    # Add colorbar outside the subplot\n",
    "    cbar = fig.colorbar(im, ax=axes.ravel().tolist(), orientation='vertical', fraction=0.05, pad=0.04)\n",
    "    cbar.set_label('Wetland Probability')\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping plotting process as wetland type model is not available.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e92ef-e1b0-4afc-98fc-fdd5ae2d9a97",
   "metadata": {},
   "source": [
    "### Independent accuaracy assessment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fdaffe-0733-4e4c-b387-9a427a161f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if type_model exists\n",
    "if 'type_model' in locals():\n",
    "# Load the testing points GeoDataFrame\n",
    "    testing_points = gpd.read_file(f'data/{prefix}_testing_samples.geojson')\n",
    "\n",
    "    # Sample the predictions and probabilities at testing points\n",
    "    sampled_predictions = []\n",
    "    sampled_probabilities = []\n",
    "\n",
    "    for point in testing_points.geometry:\n",
    "        # Extract the point coordinates\n",
    "        x, y = point.x, point.y\n",
    "\n",
    "        # Sample the binary predictions at the point coordinates\n",
    "        prediction = wetland_type_predictions.sel(x=x, y=y, method='nearest').item()\n",
    "\n",
    "        # Append the sampled values\n",
    "        sampled_predictions.append(prediction)\n",
    "\n",
    "    # Add the sampled values as new columns in the GeoDataFrame\n",
    "    testing_points['sampled_predictions'] = sampled_predictions\n",
    "    \n",
    "    # Remove rows with NaN values for sampled predictions\n",
    "    testing_points = testing_points.dropna(subset=['sampled_predictions'])\n",
    "\n",
    "    # Calculate the number of correctly classified samples\n",
    "    correct_count = (testing_points['sampled_predictions'] == testing_points['class_id']).sum()\n",
    "\n",
    "    # Calculate the total number of samples\n",
    "    total_count = len(testing_points)\n",
    "\n",
    "    # Calculate the accuracy as a percentage\n",
    "    accuracy_percentage = (correct_count / total_count) * 100\n",
    "\n",
    "    # Print the accuracy percentage\n",
    "    print(f\"Overall Accuracy: {accuracy_percentage:.2f}%\")\n",
    "else:\n",
    "    print(\"Skipping paccuracy calculation as wetland type model is not available.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c45799-be8a-40ce-b13f-2ee9f9234eba",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ec171-625f-44a6-9e07-aee0c4a771c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "datetime.today().strftime('%Y-%m-%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
