{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1165484a-69c1-4c5c-8403-96b95580f5a3",
   "metadata": {},
   "source": [
    "# Training feature selection and Random Forest classifier training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c41f7ac-4de3-40d2-a9f6-fcbd2f957e86",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "This notebook focuses on training a Random Forest classifier for wetland mapping and classification using Earth Observation (EO) measurements. Before fitting the classifier, the notebook removes highly correlated features, aiming to optimize the model's performance. Furthermore, it evaluates the model's performance using a confusion matrix, providing insights into the classification accuracy for different wetland classes. By incorporating these steps, the notebook aims to enhance the accuracy, interpretability, and reliability of the Random Forest classifier in mapping and classifying wetlands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76b8cdd-be89-492d-983f-b82d4c4371b2",
   "metadata": {},
   "source": [
    "## Fit and evaluate a Random Forest classifier\n",
    "\n",
    "We use the [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) implemented in the [`scikit-learn`](https://scikit-learn.org/stable/) Python libary to map wetland types. \n",
    "\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. \n",
    "\n",
    "A number of hyperparameters can be tuned for this classifier and we will use funtionalities provided by the [`scikit-learn`](https://scikit-learn.org/stable/) libary to tune and evaluate the model.\n",
    "\n",
    "This notebook demonstrates how to fit, tune and evaluate the wetland type classification model. We will use the subset of training features balanced and selected earlier in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a1ba9-8519-4fa9-83f4-6f311d7ed67c",
   "metadata": {},
   "source": [
    "### Load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faeb48b-aab8-483b-92b3-4a67b8c2d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import datacube\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score,cohen_kappa_score,confusion_matrix,ConfusionMatrixDisplay,balanced_accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb960991-76d0-456b-9ad6-4a2a7bad5c45",
   "metadata": {},
   "source": [
    "### Load sampled training data\n",
    "We will load the training data saved from the [feature extraction notebook](03_Feature_Extraction.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6975ddb4-24d1-42a9-8336-11e42227daca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a prefix to identify the area of interest in the saved outputs\n",
    "# By assigning the desired prefix, you can easily identify the outputs associated with the specific area of interest.\n",
    "prefix = 'KZN'\n",
    "\n",
    "training_features = gpd.read_file(f'results/{prefix}_training_features.geojson')\n",
    "\n",
    "# Check if both 'class_id_binary' and 'class_id_type' are present\n",
    "if 'class_id_binary' in training_features.columns and 'class_id_type' in training_features.columns:\n",
    "    fields = ['class_id_binary', 'class_id_type']\n",
    "elif 'class_id_binary' in training_features.columns:\n",
    "    fields = ['class_id_binary']\n",
    "elif 'class_id_type' in training_features.columns:\n",
    "    fields = ['class_id_type']\n",
    "else:\n",
    "    fields = []\n",
    "\n",
    "print(fields)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950e12d6-d403-40e1-bcaa-9652f1b5f824",
   "metadata": {},
   "source": [
    "Remove unnecessary columns e.g. coordinates and geometry attributes and remove samples with NaN values which were 0 after geomedian calculation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299c467b-98c1-477c-932f-cb688981bf5c",
   "metadata": {},
   "source": [
    "#### Training features with wetland classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f3fe6-0a2e-43a1-a914-19e341f29e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns not for analysis\n",
    "df_training_data=training_features.iloc[:,:-3]\n",
    "# remove invalid values\n",
    "# df_training_data=df_training_data.loc[(df_training_data!=0).all(axis=1)].reset_index(drop=True) \n",
    "print('Number of features: ',len(df_training_data.columns))\n",
    "\n",
    "df_training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45ee11b-60b0-4aea-9b85-c441d4e1551f",
   "metadata": {},
   "source": [
    "### Check for feature correlation \n",
    "\n",
    "When it comes to training feature selection, our approach involved extracting a wide range of Earth Observation (EO) measurements that had the potential to contribute to the mapping and classification of wetlands. However, it is important to address the issue of correlated measurements, where certain features describe similar or related properties of wetlands. Including highly correlated features in a model can increase its complexity without necessarily improving prediction performance. To mitigate this, it is considered good practice to remove highly correlated features. By doing so, we reduce the complexity of the model while maintaining or even enhancing its prediction capabilities.\n",
    "\n",
    "\n",
    "To inspect the feature correlations, we will split the data into features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9b9b43-9389-40e6-88f5-8deee47b401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into features and labels\n",
    "X = df_training_data.drop(fields, axis=1).values\n",
    "y = df_training_data[['class_id_binary']].values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd9247-6a08-4091-abe0-6d0813bdaf4d",
   "metadata": {},
   "source": [
    "#### Create and visualize the correlation_matrix\n",
    "\n",
    "The correlations can be visualized in a color-coded table. The coefficient values range from 0 to 1, with 0 for no corelation and 1 for perfect correlation. The diagonal elements are always 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f3ca79-ccee-40b8-bff3-b4221db2fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = df_training_data.drop(fields, axis=1)\n",
    "correlation_matrix = X_.corr().abs()\n",
    "\n",
    "# plot correlation matrix\n",
    "correlation_matrix.style.background_gradient(cmap='coolwarm').format(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99197747-8b5d-40dc-921c-3d23f945e65a",
   "metadata": {},
   "source": [
    "### Remove correlated features.\n",
    "We will define a function and set a threshold of 0.9 to remove correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0679d-cc12-4142-bbe7-30e30891855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removal_threshold = 0.9\n",
    "\n",
    "# # function for removing correlated variables\n",
    "# def DropCorrelatedFeatures(X_, removal_threshold=0.9):\n",
    "#     to_drop = set()  # set of features to drop\n",
    "#     correlation_matrix = X_.corr().abs()\n",
    "#     for i in range(len(correlation_matrix.columns)):\n",
    "#         for j in range(i):\n",
    "#             if (correlation_matrix.iloc[i, j] >= removal_threshold) and (correlation_matrix.columns[j] not in to_drop):\n",
    "#                 colname = correlation_matrix.columns[i]\n",
    "#                 to_drop.add(colname)\n",
    "#     to_drop = list(to_drop)\n",
    "#     print(to_drop)\n",
    "#     X_dropped = X_.copy()\n",
    "#     X_dropped = X_dropped.drop(to_drop, axis=1)\n",
    "#     return X_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c13575a-19e0-46c9-bc20-c7bb0a96bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_ = df_training_data.drop(field, axis=1)\n",
    "# X_dropped = DropCorrelatedFeatures(X_, removal_threshold=removal_threshold)\n",
    "# print(\"# of features kept:\", len(X_dropped.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617168c7-4566-42d7-bb0c-7656b51f4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_training_data = pd.concat([df_training_data[field], X_dropped], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cede07-4e9e-42fe-bd90-23110455d37e",
   "metadata": {},
   "source": [
    "## Grid search for optimal hyperparameters\n",
    "\n",
    "A number of hyperparameters can be tuned to optimize the performance of a random forest algorithm. These parameters specifiy how many decision trees are used, when and how data are split into nodes, how many samples and features are used when looking for the best split, and so on. A complete list of parameters and their explanations can be found in the [library documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier).\n",
    "\n",
    "In our example, we perform a grid search of the following four hyperparameters:\n",
    "\n",
    "* `class_weight`: Weights associated with classes. If not given, all classes are supposed to have weight one. The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as `n_samples / (n_classes * np.bincount(y))`.\n",
    "* `max_features`: The number of features to consider when looking for the best split. If “sqrt”, then `max_features=sqrt(n_features)`. If “log2”, then `max_features=log2(n_features)`. If None, then `max_features=n_features`.\n",
    "* `n_estimators`: The number of trees in the forest.\n",
    "* `criterion`: The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “log_loss” and “entropy” both for the Shannon information gain, see [Mathematical formulation](https://scikit-learn.org/stable/modules/tree.html#tree-mathematical-formulation).\\\n",
    "\n",
    "> Searching over a large grid takes time. For testing, the search can be applied on a subset of parameter options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73288ecc-62ac-4062-84e1-be738719d4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "grid_parameters={'n_estimators': [int(x) for x in np.linspace(start = 50, stop = 100, num = 2)],\n",
    "                 'max_features': ['sqrt', 'log2']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d0567-713a-4b4f-9e96-b0d5286967d4",
   "metadata": {},
   "source": [
    "#### Binary classsifier\n",
    "The provided code initializes a Random Forest classifier for binary classification, aiming to differentiate between wetland and non-wetland instances. It starts by preparing the feature matrix (X) and the corresponding binary labels (y_binary). Hyperparameter tuning is conducted using grid search and stratified K-fold cross-validation, seeking the optimal combination of parameters like the number of estimators and the maximum number of features. Once the best parameters are determined, the binary classifier is trained with these settings. This process ensures that the classifier is effectively trained to discern wetland areas from non-wetland ones based on the provided features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02285df2-dd58-4ace-80d2-f21d6e67c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise a random forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "#convert variable names into sci-kit learn nomenclature\n",
    "X = df_training_data.to_numpy()[:,2:]\n",
    "\n",
    "y_binary = df_training_data[fields[0]].to_numpy()\n",
    "\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "binary_grid_parameters ={'n_estimators': [int(x) for x in np.linspace(start = 50, stop = 100, num = 2)],\n",
    "                 'max_features': ['sqrt', 'log2']}\n",
    "\n",
    "# stratified K-fold splitting strategy for grid search\n",
    "cv=model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=0) \n",
    "\n",
    "# grid search to find optimal random forest classifier hyperparameters\n",
    "print('Grid searching best hyper-parameters for binary classification...')\n",
    "binary_grid_search =model_selection.GridSearchCV(estimator = rf,param_grid=binary_grid_parameters,cv=cv,n_jobs=-1)\n",
    "\n",
    "binary_grid_search.fit(X, y_binary)\n",
    "print('Optimal parameters for binary classification: \\n', binary_grid_search.best_params_)\n",
    "\n",
    "# Train the binary classifier with the best parameters\n",
    "binary_rf = RandomForestClassifier(**binary_grid_search.best_params_, random_state=1, n_jobs=-1)\n",
    "binary_rf.fit(X, y_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688bb588-94f2-4c84-afa5-7ee8682df764",
   "metadata": {},
   "source": [
    "#### Wetland types classifier\n",
    "Similar to the binary classifier, we define a parameter grid for hyperparameter tuning of the wetland type classifier. Then we perform grid search to find the optimal hyperparameters for the wetland type classifier using GridSearchCV. After finding the optimal parameters, we train the wetland type classifier (type_rf) using these parameters and the filtered feature matrix and target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7067072a-ef13-4708-86a2-02663493a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out only the wetland points for type classification\n",
    "wetland_indices = np.where(y_binary)[0]\n",
    "X_wetland = X[wetland_indices]\n",
    "\n",
    "# Check if the second field is available in the fields variable\n",
    "if len(fields) > 1:\n",
    "    y_wetland_type = df_training_data[fields[1]].to_numpy()[wetland_indices]\n",
    "else:\n",
    "    print(\"No wetland classes available.\")\n",
    "    # Handle the absence of wetland classes (skip further processing or raise an exception)\n",
    "\n",
    "# Remove instances labeled as non-wetland from wetland type classification\n",
    "if 'y_wetland_type' in locals():\n",
    "    non_wetland_indices = np.where(y_wetland_type != 0)[0]\n",
    "    X_wetland = X_wetland[non_wetland_indices]\n",
    "    y_wetland_type = y_wetland_type[non_wetland_indices]\n",
    "\n",
    "    # Define parameter grid for wetland type classification\n",
    "    type_grid_parameters = {'n_estimators': [int(x) for x in np.linspace(start=50, stop=100, num=2)],\n",
    "                            'max_features': ['sqrt', 'log2']}\n",
    "\n",
    "    # Grid search to find optimal hyperparameters for wetland type classification\n",
    "    print('Grid searching best hyper-parameters for wetland type classification...')\n",
    "    type_grid_search = model_selection.GridSearchCV(estimator=rf,\n",
    "                                                    param_grid=type_grid_parameters,\n",
    "                                                    cv=cv,\n",
    "                                                    n_jobs=-1)\n",
    "    type_grid_search.fit(X_wetland, y_wetland_type)\n",
    "    print('Optimal parameters for wetland type classification: \\n', type_grid_search.best_params_)\n",
    "\n",
    "    # Train the wetland type classifier with the best parameters\n",
    "    type_rf = RandomForestClassifier(**type_grid_search.best_params_, random_state=1, n_jobs=-1)\n",
    "    type_rf.fit(X_wetland, y_wetland_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c76ff7-1137-4770-9387-bda4592d6634",
   "metadata": {},
   "source": [
    "### Examine feature importance\n",
    "\n",
    "Feature importance shows which features have the largest effect on the model prediction. We will explore the top 10 most important features and visualize the importance of all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c9ecb-444f-440a-a26d-82577eb686af",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_feature_importances = binary_rf.feature_importances_\n",
    "\n",
    "# List of variable names\n",
    "variable_names = df_training_data.columns[2:]\n",
    "\n",
    "# Calculate the figure width based on the number of variables\n",
    "figure_width = max(5, len(variable_names) * 0.1)  # Minimum width of 8 inches\n",
    "\n",
    "# If type_rf is available, create subplots for binary and type classifications\n",
    "if 'type_rf' in locals():\n",
    "    # Create subplots for binary classification and wetland type classification side by side\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(2 * figure_width, 6))\n",
    "\n",
    "    # Plot feature importances for binary classification\n",
    "    order_binary = np.argsort(binary_feature_importances)[::-1]\n",
    "    order_binary = order_binary[::-1]  # Reverse order to display most important variables at the top\n",
    "    axs[0].barh(y=variable_names[order_binary], width=binary_feature_importances[order_binary], height=0.5, align='center')\n",
    "    axs[0].set_title('Feature importances for binary model', fontsize=8)\n",
    "    axs[0].set_ylabel('Variable', fontsize=6)\n",
    "    axs[0].tick_params(axis='both', which='major', labelsize=6)\n",
    "    axs[0].set_xlabel('Importance', fontsize=6)\n",
    "    axs[0].set_ylim(-0.5, len(variable_names) - 0.5)\n",
    "\n",
    "    type_feature_importances = type_rf.feature_importances_\n",
    "    # Plot feature importances for wetland type classification\n",
    "    order_type = np.argsort(type_feature_importances)[::-1]\n",
    "    order_type = order_type[::-1]  # Reverse order to display most important variables at the top\n",
    "    axs[1].barh(y=variable_names[order_type], width=type_feature_importances[order_type], height=0.5, align='center')\n",
    "    axs[1].set_title('Feature importances for wetland types model', fontsize=8)\n",
    "    axs[1].set_ylabel('Variable', fontsize=6)\n",
    "    axs[1].tick_params(axis='both', which='major', labelsize=6)\n",
    "    axs[1].set_xlabel('Importance', fontsize=6)\n",
    "    axs[1].set_ylim(-0.5, len(variable_names) - 0.5)\n",
    "\n",
    "    plt.tight_layout()  # Automatically adjust the layout\n",
    "    plt.show()\n",
    "else:\n",
    "    # If type_rf is not available, plot only the feature importances for binary classification\n",
    "    plt.figure(figsize=(figure_width, 6))\n",
    "    order_binary = np.argsort(binary_feature_importances)[::-1]\n",
    "    order_binary = order_binary[::-1]  # Reverse order to display most important variables at the top\n",
    "    plt.barh(y=variable_names[order_binary], width=binary_feature_importances[order_binary], height=0.5, align='center')\n",
    "    plt.title('Feature importances for binary model', fontsize=8)\n",
    "    plt.ylabel('Variable', fontsize=6)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=6)\n",
    "    plt.xlabel('Importance', fontsize=6)\n",
    "    plt.ylim(-0.5, len(variable_names) - 0.5)\n",
    "    plt.tight_layout()  # Automatically adjust the layout\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c67d50-b27b-4780-9ff8-474afcab2712",
   "metadata": {},
   "source": [
    "## Accuracy assessment\n",
    "\n",
    "Finally we use a 5-fold cross validation to estimate the performance of the model. While the cross validation evaluates the predictive power of our model on unseen data, the result may not generalize for input data that have distinct spectral and temporal signatures to the training dataset. \n",
    "\n",
    "We will inspect four performance metrics:\n",
    "\n",
    "* Overall accuracy\n",
    "* Precision for each class - fraction of positive predictions that are correct. $precision = N_{True Positive}/(N_{True Positive} + N_{False Positive})$. The best value is 1 and the worst value is 0.\n",
    "* Recall for each class - ability of the classifier to find all the positive samples. $recall = N_{True Positive}/(N_{True Positive} + N_{False Negative})$. The best value is 1 and the worst value is 0.\n",
    "* F1 macro - averaged F1 scores for all classes. For each class, $F1 = 2*(precision*recall)/(precision+recall)$\n",
    "* Plot confusion matrix - The confusion matrix compares all predictions against the labels and allow us to see what types of prediction errors the model is making. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203a118-4cfb-467d-8263-a4e4982ce1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the class label dictionary for wetland types\n",
    "with open(f'data/{prefix}_labels_dict.json', 'r') as json_file:\n",
    "    labels_dict_type = json.load(json_file)\n",
    "    \n",
    "# Remove the non-wetland class from the dictionary\n",
    "if 0 in labels_dict_type.values():\n",
    "    del labels_dict_type['Non-wetland']    \n",
    "\n",
    "# Create a new label dictionary for binary classification\n",
    "labels_dict_binary = {'Non-wetland': 0, 'Wetland': 1}\n",
    "\n",
    "# Stratified k-fold splitting\n",
    "skf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=1) \n",
    "\n",
    "# Binary classification evaluation\n",
    "print(\"Binary Classification Evaluation:\")\n",
    "overall_acc_binary = model_selection.cross_val_score(binary_rf, X, y_binary, cv=skf, scoring='accuracy')\n",
    "print('Overall accuracy from cross-validation (Binary): ', np.mean(overall_acc_binary))\n",
    "\n",
    "predictions_binary = model_selection.cross_val_predict(binary_rf, X, y_binary, cv=skf)\n",
    "precision_binary = precision_score(y_binary, predictions_binary, labels=list(labels_dict_binary.values()), average=None)\n",
    "print('Precision for each class (Binary): \\n', dict(zip(list(labels_dict_binary.keys()), np.around(precision_binary, 3))))\n",
    "\n",
    "recall_binary = recall_score(y_binary, predictions_binary, labels=list(labels_dict_binary.values()), average=None)\n",
    "print('Recall for each class (Binary): \\n', dict(zip(list(labels_dict_binary.keys()), np.around(recall_binary, 3))))\n",
    "\n",
    "f1_macro_binary = model_selection.cross_val_score(binary_rf, X, y_binary, cv=skf, scoring='f1_macro')\n",
    "print('F1_macro from cross-validation scores (Binary): ', np.mean(f1_macro_binary))\n",
    "\n",
    "cm_binary = confusion_matrix(y_binary, predictions_binary)\n",
    "disp_binary = ConfusionMatrixDisplay(confusion_matrix=cm_binary, display_labels=list(labels_dict_binary.keys()))\n",
    "print('Confusion matrix (Binary):\\n')\n",
    "fig_binary, ax_binary = plt.subplots(figsize=(8, 6))\n",
    "disp_binary.plot(ax=ax_binary)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Define the file paths for saving the feature names\n",
    "binary_features_path = f\"results/{prefix}_binary_model_features.json\"\n",
    "# Create dictionaries to store the feature names\n",
    "binary_features_dict = {}\n",
    "# Store binary model feature names\n",
    "binary_features_dict[\"features\"] = list(variable_names)\n",
    "# Save the binary model feature names to a JSON file\n",
    "with open(binary_features_path, \"w\") as binary_file:\n",
    "    json.dump(binary_features_dict, binary_file, indent=4)\n",
    "\n",
    "# Save the binary classification model\n",
    "dump(binary_rf, f'results/{prefix}_Binary_RF_model.joblib')\n",
    "\n",
    "# Wetland type classification evaluation if type_rf exists\n",
    "if 'type_rf' in locals():\n",
    "    print(\"\\nWetland Type Classification Evaluation:\")\n",
    "    overall_acc_type = model_selection.cross_val_score(type_rf, X_wetland, y_wetland_type, cv=skf, scoring='accuracy')\n",
    "    print('Overall accuracy from cross-validation (Wetland Type): ', np.mean(overall_acc_type))\n",
    "\n",
    "    predictions_type = model_selection.cross_val_predict(type_rf, X_wetland, y_wetland_type, cv=skf)\n",
    "    precision_type = precision_score(y_wetland_type, predictions_type, labels=list(labels_dict_type.values()), average=None)\n",
    "    print('Precision for each class (Wetland Type): \\n', dict(zip(list(labels_dict_type.keys()), np.around(precision_type, 3))))\n",
    "\n",
    "    recall_type = recall_score(y_wetland_type, predictions_type, labels=list(labels_dict_type.values()), average=None)\n",
    "    print('Recall for each class (Wetland Type): \\n', dict(zip(list(labels_dict_type.keys()), np.around(recall_type, 3))))\n",
    "\n",
    "    f1_macro_type = model_selection.cross_val_score(type_rf, X_wetland, y_wetland_type, cv=skf, scoring='f1_macro')\n",
    "    print('F1_macro from cross-validation scores (Wetland Type): ', np.mean(f1_macro_type))\n",
    "\n",
    "    cm_type = confusion_matrix(y_wetland_type, predictions_type)\n",
    "    disp_type = ConfusionMatrixDisplay(confusion_matrix=cm_type, display_labels=list(labels_dict_type.keys()))\n",
    "    print('Confusion matrix (Wetland Type):\\n')\n",
    "    fig_type, ax_type = plt.subplots(figsize=(8, 6))\n",
    "    disp_type.plot(ax=ax_type)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    # Define the file paths for saving the feature names\n",
    "    type_features_path = f\"results/{prefix}_type_model_features.json\"\n",
    "    # Create dictionaries to store the feature names\n",
    "    type_features_dict = {}\n",
    "    # Store type model feature names\n",
    "    type_features_dict[\"features\"] = list(variable_names)\n",
    "    # Save the type model feature names to a JSON file\n",
    "    with open(type_features_path, \"w\") as type_file:\n",
    "        json.dump(type_features_dict, type_file, indent=4)\n",
    "\n",
    "    # Save the wetland type classification model\n",
    "    dump(type_rf, f'results/{prefix}_Type_RF_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d860b7-e925-4e13-b8d6-3f34e3c2f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(binary_features_path):\n",
    "    with open(binary_features_path, \"r\") as binary_file:\n",
    "        binary_features_dict = json.load(binary_file)\n",
    "    binary_feature_names = binary_features_dict[\"features\"]\n",
    "    print(type(binary_feature_names))\n",
    "    print(\"Loaded binary model features.\")\n",
    "else:\n",
    "    print(\"No binary model features found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcee385-e821-4972-8196-f86693b9ec7b",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Africa data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks).\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
